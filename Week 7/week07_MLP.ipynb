{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-34f9557d-8f15-494e-8d65-74c8ae429c5c",
    "deepnote_cell_type": "markdown",
    "id": "6Og4DnJPrB4A"
   },
   "source": [
    "# Week 7 - Support Vector Machines\n",
    "\n",
    "### Aims\n",
    "\n",
    "By the end of this notebook you will be able to understand \n",
    "\n",
    ">* the Separable vs Non-separable data\n",
    ">* the model refinement for SVM\n",
    ">* the binary case for Default data \n",
    "\n",
    "\n",
    "1. [Setup](#setup)\n",
    "\n",
    "2. [Separable and Non separable Data Cases](#RBH)\n",
    "\n",
    "3. [Model refinement](#refine)\n",
    "\n",
    "4. [Default Data for Binary Example](#default)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdHUSbWsvZ7h"
   },
   "source": [
    "- In this WS we will be exploring the basics of support vector machine models. \n",
    "- SVMs have their own entire submodule of sklearn which includes more than we will be able to cover in this workshop. - We will be focusing on the most straight forward case, which is a support vector machine classifier which is provide by sklearn as the SVC model. For the details please have a look at https://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "NOTE THAT, for the simplicity we did not use any data partitioning in below for toy data examples. But for the real data sets, we have the data splitting procedure as a general procedure (Default data example). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-645a25eb-6010-425a-88c0-ecf0093a9edc",
    "deepnote_cell_type": "markdown",
    "id": "6sVlUI4SvZ7i"
   },
   "source": [
    "# 1. General Setup <a id='setup'></a>\n",
    "\n",
    "## 1.1 Packages\n",
    "\n",
    "Now lets load in the packages you wil need for this workshop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-d0af5d8f-8894-4c5a-b754-353993666790",
    "deepnote_cell_type": "code",
    "id": "grVNp8GrrH0g",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "# Display plots inline\n",
    "%matplotlib inline  \n",
    "\n",
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# sklearn modules list that might be useful, maybe you do not need to use all of them\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC, LinearSVC           # SVM\n",
    "from mpl_toolkits.mplot3d import Axes3D          # 3d plots\n",
    "from sklearn.preprocessing import StandardScaler # scaling features\n",
    "from sklearn.preprocessing import LabelEncoder   # binary encoding\n",
    "from sklearn.pipeline import Pipeline            # combining classifier steps\n",
    "from sklearn.preprocessing import PolynomialFeatures # make PolynomialFeatures\n",
    "from sklearn.datasets import make_classification, make_moons  # make example data\n",
    "import warnings # prevent warnings\n",
    "import joblib # saving models\n",
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold\n",
    "from scipy.stats.distributions import uniform, loguniform\n",
    "import itertools\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "#  from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "# from imblearn.metrics import classification_report_imbalanced\n",
    "import re\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# For loading SVC from sklearn\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting defaults\n",
    "plt.rcParams['figure.figsize'] = (8,8)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "plt.rcParams['lines.markersize'] = 7.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8_vjOPKdqLm"
   },
   "source": [
    "##  1.2 Helper Functions\n",
    "\n",
    "Below are helper functions we will be using in this workshop. You can create your own if you think it is necessary OR directly use already available helper functions within `sklearn library`.  \n",
    "\n",
    "- `plot_margin()`: visualization of margins in figures.\n",
    "\n",
    "You can modify the following function based on your needs as well. These practices would be important while you are working on your project either. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3LgZwfrdyq7"
   },
   "outputs": [],
   "source": [
    "# About visualization of margins in figures\n",
    "def plot_margin(model, data, x='x', y='y', cat='z', show_support_vectors = True, nx=50, ny=50):\n",
    "    # Plot the data\n",
    "    p = sns.scatterplot(x=x, y=y, hue=cat, data=data, legend=False)\n",
    "    \n",
    "    # Find the extent of x and y\n",
    "    xlim = p.get_xlim()\n",
    "    ylim = p.get_ylim()\n",
    "    \n",
    "    # Create a grid of points\n",
    "    xx = np.linspace(xlim[0], xlim[1], nx)\n",
    "    yy = np.linspace(ylim[0], ylim[1], ny)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    \n",
    "    # Calculate the label for each point in the grid\n",
    "    xy = np.c_[XX.ravel(), YY.ravel()]\n",
    "    Z = model.decision_function(xy).reshape(XX.shape)\n",
    "    \n",
    "    # plot contours of decision boundary and margins\n",
    "    p.contour(XX, YY, Z, colors='k', \n",
    "              levels=[-1, 0, 1], alpha=0.5,\n",
    "              linestyles=['--', '-', '--'])\n",
    "\n",
    "    # highlight support vectors\n",
    "    if (show_support_vectors):\n",
    "        p.scatter(model.support_vectors_[:, 0], \n",
    "                  model.support_vectors_[:, 1], s=100,\n",
    "                  linewidth=1, facecolors='none', edgecolors='k')\n",
    "\n",
    "    # Show confusion table in the title\n",
    "    p.set_title(\n",
    "        \"TN: {0}, FP: {1}, FN: {2}, TP: {3}\".format(\n",
    "            *confusion_matrix(\n",
    "                data[cat],\n",
    "                m.predict(data.drop(cat, axis=1))\n",
    "            ).flatten()\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **__REMARK__**\n",
    "\n",
    "- The implementation of the SVC with sklearn requires a slightly different meaning for the parameter C. Please note that, \"Regularization parameter. \n",
    "- The strength of the regularization is inversely proportional to C. Must be strictly positive.\" and \"The C parameter trades of correct classification of training examples against maximization of the decision function's margin. \n",
    "- For larger values of C, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. \n",
    "- A lower C will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy. In other words C behaves as a regularization parameter in the SVM.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Difference between SVC and LinearSVC**\n",
    "\n",
    "The linear models `LinearSVC()` and `SVC(kernel='linear')` yield slightly different decision boundaries. This can be a consequence of the following differences:\n",
    "\n",
    ">- `LinearSVC` minimizes the squared hinge loss while SVC minimizes the regular hinge loss.\n",
    ">- `LinearSVC` uses the One-vs-All (also known as One-vs-Rest) multiclass reduction while `SVC` uses the One-vs-One multiclass reduction.\n",
    ">- In terms of graphical display, note that unlike `SVC` (based on LIBSVM), `LinearSVC` (based on LIBLINEAR) does not provide the support vectors.\n",
    "\n",
    "For further details, try to compare differences from their documentations\n",
    "\n",
    "- `SVC` with `linear` kernel selection: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "- `LinearSVC` function: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-90709695-8746-4669-9199-fd144a6ec872",
    "deepnote_cell_type": "markdown",
    "id": "yz3bjxcbvZ7r"
   },
   "source": [
    "# 2 Separable Data <a id='RBH'></a>\n",
    "\n",
    "We will begin by examining several toy data problems to explore the basics of these models. To begin we will read in data for the first example from ex1.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eF19U6ivvZ7r"
   },
   "outputs": [],
   "source": [
    "ex1 = pd.read_csv(\"ex1.csv\")\n",
    "ex1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izo4A3SSvZ7t"
   },
   "source": [
    "We can see the that data is composed of two classes in two dimensions, and it is clear that these two classes are perfectly linearly separable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "id": "r3QIcTBsCZAf",
    "outputId": "5c544249-18e4-4129-be03-fc2bc14f45cb"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x='x', y='y', hue='z', data=ex1, legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-da6d5f4b-bf6b-43f7-9751-a015e6a9924e",
    "deepnote_cell_type": "markdown",
    "id": "50AbKP76vZ7u"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 1 (CORE)\n",
    "\n",
    "Like the other models we've already seen, we fit the SVM by constructing our feature matrix and outcome vector and then calling the fit method for our model object;\n",
    "\n",
    "1. Separate the features and outcome in the toy dataset ex1.csv\n",
    "2. Fit a SVC model for this data set using `SVC()` function (Note that you need to change the default value of kernel and parameter C)\n",
    "3. Visualize the decision boundary and the margins using the plot_margin function we defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-4d9907ea-c7c7-4c16-aee7-5e12c3e00cc1",
    "deepnote_cell_type": "code",
    "id": "-mG-rvtrvZ7u",
    "output_cleared": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-a3d1594c-000b-48dd-b347-dfa5410fe0a7",
    "deepnote_cell_type": "markdown",
    "id": "01HS3aS8vZ7u"
   },
   "source": [
    "---\n",
    "\n",
    "**!!! Add your text solution here !!!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚩 Exercise 2  (CORE)\n",
    "\n",
    "Based on the results of previous exercise, state that\n",
    "\n",
    "- How many support vectors are there for this model?\n",
    "- How does the boundary line and the margins change as you change the value of C?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKglNm6mD_jx"
   },
   "source": [
    "# 3 Non-Separable Data\n",
    "\n",
    "We will not complicate our previous example somewhat by adding two additional points from the blue A class to our data. This is available in the ex2.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "id": "7XEqi56tEHsB",
    "outputId": "d9414ca1-3126-4d20-ad0f-dd5602238d03"
   },
   "outputs": [],
   "source": [
    "ex2 = pd.read_csv(\"ex2.csv\")\n",
    "print(ex2.head())\n",
    "\n",
    "# To visualize\n",
    "sns.scatterplot(x='x', y='y', hue='z', data=ex2, legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6TMeCFgAJVt"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 3  (CORE)\n",
    "\n",
    "- Fit a SVC model to these data using the same code we used with example 1.\n",
    "- How does the \"fit\" of this model differ compared to the \"fit\" for example 1. Hint - make your comparison for equivalent values of C.\n",
    "- How do the boundary line and margins change as you change the value of C?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xhjuXdcvZ7v"
   },
   "source": [
    "---\n",
    "\n",
    "**!!! Add your comments about the answer here !!!**\n",
    "\n",
    "\n",
    "\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YZt5su8C7WZ"
   },
   "source": [
    "# 4  Non-linear Case\n",
    "\n",
    "Next we will look at a new data set that would seem to also fall in the non-separable category. The data set that we are using is ex3.csv now\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "id": "jSwzAMONFJm1",
    "outputId": "229a9a69-2e51-4ff8-b902-ac52fc25c439"
   },
   "outputs": [],
   "source": [
    "# For the new data set \n",
    "ex3 = pd.read_csv(\"ex3.csv\")\n",
    "\n",
    "# To visualize\n",
    "sns.scatterplot(x='x', y='y', hue='z', data=ex3, legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00022-28a7a9d1-e95b-47b7-a31c-9b247c628087",
    "deepnote_cell_type": "markdown",
    "id": "mvrOq4afvZ7z"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 4  (CORE)\n",
    "\n",
    "For this data we will consider a simple polynomial kernel with degree 2 \n",
    "(choose first C = 1 ) and visualize the margins using `plot_margin()` again\n",
    "\n",
    "More details on the various kernels that can be used with the SVC model are available \n",
    "https://scikit-learn.org/stable/modules/svm.html#svm-kernels\n",
    "\n",
    "**The kernel function can be any of the following:**\n",
    "\n",
    "- linear : $\\langle x, x'\\rangle$\n",
    "- polynomial : $(\\gamma \\langle x, x'\\rangle + r)^d$\n",
    "- rbf : $\\exp(-\\gamma \\|x-x'\\|^2)$ where  $\\gamma$ is specified by parameter gamma, must be greater than 0\n",
    "- sigmoid : $\\tanh(\\gamma \\langle x,x'\\rangle + r)$ where $r$ is specified by `coef0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2qXwFXvQbBq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00022-28a7a9d1-e95b-47b7-a31c-9b247c628087",
    "deepnote_cell_type": "markdown",
    "id": "ZecdeX9yQdVh"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 5  (CORE)\n",
    "\n",
    "Based on the above fitted models\n",
    "\n",
    "1. Compare the fit of the model using the polynomial and linear kernel. Describe the shape of the boundaries and the margins.\n",
    "\n",
    "2. How do the boundary line and margins change as you change the value of C?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00023-04722d32-f420-4104-b25b-37656a71df76",
    "deepnote_cell_type": "code",
    "id": "3Mxs7ORhvZ71",
    "output_cleared": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLaXXgVqGQrd"
   },
   "source": [
    "## 4.1 Other Kernels\n",
    "\n",
    "Next we will consider an even more complicated separation task where one class is split into two separate clusters by the second class. The data ara available as `ex4.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "id": "fAZUQs23GXAl",
    "outputId": "5634934d-e3f0-4972-8e54-d5b12da535d3"
   },
   "outputs": [],
   "source": [
    "# For the new data set \n",
    "ex4 = pd.read_csv(\"ex4.csv\")\n",
    "\n",
    "# To visualize\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x='x', y='y', hue='z', data=ex4, legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00022-28a7a9d1-e95b-47b7-a31c-9b247c628087",
    "deepnote_cell_type": "markdown",
    "id": "z4NLHMUEdKxP"
   },
   "source": [
    "---\n",
    " \n",
    "### 🚩 Exercise 6  (CORE)\n",
    "\n",
    "Set up a function for experimenting with different penalties and kernel functions for this dataset (ex4.csv). For this purpose, consider, \n",
    "\n",
    "1. **C** in $[1,5,10,50,100]$\n",
    "2. **degree** in $[2,3,4]$\n",
    "3. **kernel** in $['poly', 'rbf', 'linear'])$\n",
    "\n",
    "inside of the `SVC()` function. Note that the degree value is only used by polynomial kernel and is ignored by the linear and rbf kernels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9vTpIe932BFz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00022-28a7a9d1-e95b-47b7-a31c-9b247c628087",
    "deepnote_cell_type": "markdown",
    "id": "wYXQZYkTMy7M"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 7  (CORE)\n",
    "\n",
    "Based on the above fitted models\n",
    "\n",
    "1. What combination of parameters appears to produce the best fit? Is it easy to tell this by visual inspection alone?\n",
    "\n",
    "2. How do the support vectors change as the kernel, penalty, and degree are changed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uoEYNyw0Jfm"
   },
   "source": [
    "# 5  Model Assessment <a id='refine'></a>\n",
    "\n",
    "So far we have only inspected the various models by eye to get a sense of how well they fit our data. Since we are undertaking a classification task here we would like to be able to leverage the metrics and scoring tools we have already learned around logistic regression and related tools. The issue is that while we could generate a simple confusion matrix for our models' predictions this is somewhat limiting.\n",
    "\n",
    "**WARNING:** \n",
    "\n",
    "- By default, SVM models do not support the construction of anything like a ROC curve since the predictions are not probabilistic - i.e. labels are assigned based on which side of the separator a point falls. \n",
    "- As such, SVC models do not implement predict_proba by default\n",
    "- Just to recall, these are some metrics that we discussed before\n",
    "\n",
    "$$\n",
    "\\text{FPR} = \\frac{\\text{FP}}{\\text{FP}+ \\text{TN}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{\\text{TP}}{\\text{TP}+ \\text{FN}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{\\text{TP}}{\\text{TP}+ \\text{FP}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "F1 = 2\\left(\\frac{Precision \\times Recall}{Precision + Recall}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Szq6fbGf05A9"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 8  (CORE)\n",
    "\n",
    "Based on the best model that you visualized above,\n",
    "\n",
    "1. Report the accuracy of the model\n",
    "2. Obtain the confusion matrix and interpret the results in terms of the quantities defined below \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQrDeC8O8-Nf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ruvl0mkhF8xL"
   },
   "source": [
    "**!!! Add your comments about the model performance here !!!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7foBrtx5DXO"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 9  (CORE)\n",
    "\n",
    "With the available scoring metrics we can make use of the usual cross valiation tools such as cross_val_score which can be useful for the purpose of comparing different models,\n",
    "\n",
    "1. Run the following code and get the result\n",
    "2. Rearrange the ingridients of th code given below for 2-degree poylnomial kernel, compare the result with the output of 1. \n",
    "3. If you adjust $C$ are you able to find a better performing version of either the rbf or poly SVM models?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClWZ7isN5ekk"
   },
   "outputs": [],
   "source": [
    "# Define your X_ex4 and y_ex4 based on the data set first !\n",
    "\n",
    "rbf = sklearn.model_selection.cross_val_score(\n",
    "    SVC(kernel='rbf', C=1, gamma='scale'), \n",
    "    X_ex4, y_ex4, \n",
    "    cv=KFold(5, shuffle=True, random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqZpAT1MxolL"
   },
   "source": [
    "---\n",
    "\n",
    "**!!! Add your comments here !!!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjuQg3ty7XYG"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 10  (CORE)\n",
    "\n",
    "- Construct a full cross validated grid search over the parameter values: \n",
    "\n",
    "$C = np.linspace(0.1, 10, 100)$, degree = $[2,3,4]$, and kernel = $['poly', 'rbf', 'linear']$.\n",
    "\n",
    "- Which SVM model performs best? Use plot_margin to show the resulting seperator and support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "AMIeAU0kjec1"
   },
   "outputs": [],
   "source": [
    "cv = GridSearchCV(\n",
    "    SVC(),\n",
    "    param_grid = { \n",
    "        'kernel':________________, \n",
    "        'C': _______________,\n",
    "        'degree': [2,3,4]\n",
    "    },\n",
    "    cv = KFold(5, shuffle = True, random_state = 42)\n",
    ")\n",
    "\n",
    "# Fit the model on ex4 data set\n",
    "\n",
    "# Get the best model parameters and the accuracy of the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-90709695-8746-4669-9199-fd144a6ec872",
    "deepnote_cell_type": "markdown",
    "id": "YwCfjeYB4tVM"
   },
   "source": [
    "# 6 Default Data Case (EXTRA) <a id='default'></a>\n",
    "\n",
    "The dataset consists of 10000 individuals and whether their credit card has defaulted or not. Below is the column description: The main aim is to build the model using Logistic Regression and predict the accuracy of it. The included columns in the data set are as follows:\n",
    "\n",
    "* `default` - Whether the individual has defaulted\n",
    "\n",
    "* `student` - Whether the individual is the student\n",
    "\n",
    "* `balance` - The balance in the individual's account\n",
    "\n",
    "* `income` - Income of an individual\n",
    "\n",
    "We read the data into python using pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "SlbaLT675J8g",
    "outputId": "fa3accef-409c-41d9-b61d-2ac6b5d8f4b6"
   },
   "outputs": [],
   "source": [
    "df_default = pd.read_csv(\"Default.csv\", index_col=0)\n",
    "\n",
    "# for now lets just drop the student varible.\n",
    "df_default = df_default.drop(\"student\", axis=1)\n",
    "df_default.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAqFPdjr50B-"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 11 (EXTRA)\n",
    "\n",
    "1. Split the data into training and test sets (**Is there anything you should try account for when splitting the data ?**) Use the test size as $10\\%$ of the whole sample\n",
    "\n",
    "2. Convert your response variable into the numerical format\n",
    "\n",
    "3. Use the following function to get a RandomizedSearch results and sort your model results in terms of the value of \"mean_test_recall\". Comment on the obtained result in terms of accuracy and recall. Notice that the function uses `LinearSVC()` below but possible to write a similar oen using `SVC()` with suitable ingridients. \n",
    "\n",
    "Note that you can face with some warnings so try to examine those by searching the possible reasons on the use of `LinearSVC` below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nk4JacVm6qjL"
   },
   "outputs": [],
   "source": [
    "C_list = []\n",
    "pwr = -5\n",
    "for i in range(11):\n",
    "    C_list.append(2**pwr)\n",
    "    pwr+=2\n",
    "\n",
    "linear_svm = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svm_clf\", LinearSVC(random_state=42))\n",
    "    ])\n",
    "# specify parameters and distributions to sample from\n",
    "lin_param_dist = {'svm_clf__C':loguniform(C_list[0], C_list[-1])}\n",
    "\n",
    "lin_rs = RandomizedSearchCV(linear_svm, lin_param_dist, n_iter=60, \n",
    "                            scoring = [\"accuracy\", \"f1\",\"recall\",\"precision\"], \n",
    "                            cv = StratifiedKFold(n_splits=5),\n",
    "                            refit = \"recall\", \n",
    "                            random_state = 42,\n",
    "                            return_train_score = True)\n",
    "\n",
    "lin_rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00022-28a7a9d1-e95b-47b7-a31c-9b247c628087",
    "deepnote_cell_type": "markdown",
    "id": "WdZKHECXTmgU"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 12 (EXTRA)\n",
    "\n",
    "Using the following code snippet, try different values of kernel and C, what seems to produce the best model? This is again written in terms of `SVC()` function for the simplicity.  \n",
    "\n",
    "(Hint: Recommended kernels are rbf, poly, and linear)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjE0dt7Kjec3"
   },
   "outputs": [],
   "source": [
    "# Following not needed, but makes life easier\n",
    "gs_svc = sklearn.model_selection.GridSearchCV(\n",
    "    make_pipeline(\n",
    "        sklearn.preprocessing.StandardScaler(),\n",
    "        sklearn.svm.SVC()\n",
    "    ),\n",
    "    [\n",
    "        {\n",
    "            # Other kernels\n",
    "            \"svc__kernel\": ______________,\n",
    "            \"svc__C\": np.logspace(-2, 2, num=10)\n",
    "        },\n",
    "        {\n",
    "            # For polynomial part only\n",
    "            \"svc__kernel\": [\"poly\"],\n",
    "            \"svc__degree\": [1, 2, 3],\n",
    "            \"svc__C\": np.logspace(-2, 2, num=10)\n",
    "        }\n",
    "    ],\n",
    "\n",
    "    # Setting a random_state has no effect when shuffle is False. \n",
    "    # You should leave random_state to its default (None), or set shuffle=True.\n",
    "    cv = sklearn.model_selection.KFold(5, shuffle=True, random_state=1234)\n",
    ").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00022-28a7a9d1-e95b-47b7-a31c-9b247c628087",
    "deepnote_cell_type": "markdown",
    "id": "RaxRXbcF-5EX"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 13 (EXTRA)\n",
    "\n",
    "Comment out the line of code that includes the `StandardScaler` in the pipeline below. \n",
    "\n",
    "- What happens to the models predictive performance? \n",
    "\n",
    "- Try adjusting C and or kernel manually to see if you can improve the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "upqmGHZ_jec4"
   },
   "outputs": [],
   "source": [
    "m_svc = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        SVC(C=C, kernel=kernel)\n",
    "    )\n",
    "\n",
    "# fitted model\n",
    "m_svc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00022-28a7a9d1-e95b-47b7-a31c-9b247c628087",
    "deepnote_cell_type": "markdown",
    "id": "Zvf9Dact_xwz"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 14 (EXTRA)\n",
    "\n",
    "Using the following lines of codes to try different values of C to tune the model, how does its performance compare to the `SVC` model result ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1-CTLwlTaqL"
   },
   "outputs": [],
   "source": [
    "m_lsvc = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        LinearSVC(C = C, max_iter = 5000)\n",
    "    )\n",
    "\n",
    "m_lsvc(X_train,y_train)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week08.ipynb",
   "provenance": []
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "2f0f4e4a-50b4-476a-ac32-ea3a1e98d30c",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
