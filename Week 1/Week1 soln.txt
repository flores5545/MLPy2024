###week01_ex1_start_py
train_df.info()
###week01_ex1_switch_md
Most of the columns are in `float64` format, except the quality column, which is `int64`.
###week01_ex1_end

###week01_ex2_start_py
train_df.describe()
###week01_ex2_switch_md
Some of the maximum values seem quite large comparative to the median and may warrent looking into more (e.g. `total sulfur dioxide`).
###week01_ex2_end

###week01_ex3_start_py
sns.set(rc={'figure.figsize': (14, 8)}) 
sns.countplot(x = y_train)
plt.title("Figure 1. Quality of the wine")
plt.show()
###week01_ex3_switch_md
It appears the majority of wine is of medium quality, with there being much less poor (3 & 4) and good (7 & 8) quality wine.
###week01_ex3_end

###week01_ex4_start_py
sns.heatmap(X_train.corr(), annot=True, fmt='.2f', linewidths=2)
plt.title("Figure 2. Correlation Heatmap")
plt.show()
###week01_ex4_switch_md
From figure 2 we can see:
- A strong positive (>0.5) correlation between fixed acidity and citric acid/density.
- A strong negative (<-0.5) correlation of pH with acidity and citric acid. 
- A strong negative correlation of volatile acidity with citric acid. 
- A strong negative correlation between alcohol percentage and density.

There are also several moderate to low correlations present in the data as well.
###week01_ex4_end

###week01_ex5_start_py
import scipy.stats as stats

# from Python Feature Engineering Cookbook
def diagnostic_plots(df, variable): 
    plt.figure(figsize=(15,6)) 
    plt.subplot(1, 2, 1) 
    df[variable].hist(bins=30)
    plt.title("Histogram")
    plt.subplot(1, 2, 2)
    stats.probplot(df[variable], dist="norm", plot=plt) 
    plt.suptitle("Diagnostic Plots for {}".format(variable))
    plt.show()

for variable_name in X_train.columns:
    diagnostic_plots(X_train, variable_name)
###week01_ex5_switch_md
From the figures above, we can see that most varibles (apart from `density`, `pH`, and `quality`) are at least partly positively (or right) skewed. This can also be further demonstrated by the table below.
###week01_ex5_switch_py
from scipy.stats import skew 

for i, variable_name in enumerate(X_train):
    skew_val = pd.DataFrame([skew(X_train[variable_name])], index = [variable_name], columns=["skewness"])
    
    if i == 0:
        skew_df = skew_val.T
    else:
        skew_df = pd.concat([skew_df, skew_val.T], axis=1)

skew_df
###week01_ex5_end

###week01_ex6_start_py
print("There are {} duplicated observations".format(X_train.duplicated().sum()))
###week01_ex6_end

###week01_ex7_start_py
def drop_duplicated(X,y):
    df = pd.concat([X,y], axis=1)
    df = df.drop_duplicates()
    return df.iloc[:,:-1], df.iloc[:,-1]

X_train_, y_train_ = drop_duplicated(X_train, y_train)

print(X_train.shape)
print(y_train.shape)
print(X_train_.shape)
print(y_train_.shape)
###week01_ex7_end

###week01_ex8_start_py
X_train_, y_train_ = duplicated_sampler.fit_resample(X_train, y_train)

print(X_train.shape)
print(y_train.shape)
print(X_train_.shape)
print(y_train_.shape)
###week01_ex8_end

###week01_ex9_start_py
X_train.isnull().sum()
###week01_ex9_switch_md
None of the columns have a null value so we do not need to deal with them. We could fill the missing values in (e.g. mean, median, zero, ...) or drop their rows (or columns).
###week01_ex9_end

###week01_ex10_start_py
def drop_na(X,y, axis=0):
    df = pd.concat([X,y], axis=1)
    df = df.dropna(axis=axis)
    return df.iloc[:,:-1], df.iloc[:,-1]

na_sampler = FunctionSampler(func=drop_na,    # our custom function
                             validate=False)  # prevents both inputs being changed to numpy arrays
X_train_, y_train_ = na_sampler.fit_resample(X_train, y_train)

print(X_train.shape)
print(y_train.shape)
print(X_train_.shape)
print(y_train_.shape)
###week01_ex10_end

###week01_ex11_start_py
from feature_engine.selection import DropCorrelatedFeatures
import numpy as np

fs = DropCorrelatedFeatures(variables=None, 
                            method='pearson', 
                            threshold=0.6)

X_train_fs = fs.fit_transform(X_train)

print("Correlated Feature Groups")
display(fs.correlated_feature_sets_)

print("Dropped")
list(np.setdiff1d(list(X_train.columns), list(X_train_fs.columns)))
###week01_ex11_switch_md
As listed above, 4 features were dropped. In the first instance, `citric acid`, `density`, and `pH` were dropped as they were correlated to `fixed acidity` and this variable appears in the data before the others. Similarly, as `free sulfur dioxide` is before `total sulfur dioxide`, the latter was also dropped. 

`feature_engine` provides another way of dropping correlated features (`SmartCorrelatedSelection`). This method uses a number of heuristics to decide which feature to drop instead of only keeping the "first found".
###week01_ex11_end

###week01_ex12_start_md
Feature engineering requires a _"transformer"_ class as we only need to alter the features and not the number of observations.
###week01_ex12_end

###week01_ex13_start_py
from feature_engine.transformation import LogTransformer

not_skewed = ["density", "pH", "quality"]
skewed = [variable for variable in list(X_train.columns) if variable not in not_skewed]
lt = LogTransformer(variables = skewed)

train_lt = lt.fit_transform(X_train+0.001) # to stop the 0 value error

for variable in skewed:
    diagnostic_plots(train_lt, variable)
###week01_ex13_switch_md
From the diagnositic plots above you can see that it has improved some features (e.g. `volatile acidity`) but not others. For the sake of this workshop, we're not going to worry too much about this.

Using the handy conversation function `sklearn.preprocessing.FunctionTransformer` and `np.log1p`, we can actually quite easily make a transformer that applies a log transformation to your data. 
###week01_ex13_switch_py
import numpy as np
from sklearn.preprocessing import FunctionTransformer
Log1pTransformer = FunctionTransformer(np.log1p)
###week01_ex13_end

###week01_ex14_start_py
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler() 

scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)

print("Mean")
display(scaler.mean_)
print("Std")
display(scaler.scale_)
###week01_ex14_end

###week01_ex15_start_py
def standard(X):
    return (X - X.mean()) / X.std(ddof=0)

StandardTransformer = FunctionTransformer(standard)
X_train_scaled = StandardTransformer.fit_transform(X_train)
###week01_ex15_end

###week01_ex16_start_py
from sklearn.dummy import DummyRegressor

dummy = DummyRegressor()
dummy.fit(X_train, y_train)

print("Training Score")
print(round(dummy.score(X_train, y_train), 3))
print("Validation Score")
print(round(dummy.score(X_val, y_val), 3))
###week01_ex16_switch_md
The dummy model that just predicts the mean would have a $R^2$ of 0 on the training set. This because $R^2$ is effectively just measuring how good the model is compared to a horizontal line, and that is all the dummy model is doing here with the data mean.

The validation score is a minus because, for this data, the mean of the training set is not as good a measure of the data variance as the mean of the validation set.
###week01_ex16_end

###week01_ex17a_start_py
from sklearn.preprocessing import PolynomialFeatures

reg = LinearRegression()
poly = PolynomialFeatures(include_bias=False)

poly_reg_pipe = Pipeline([
    ("drop_duplicated", DD),
    ("make_dataframe", DT),
    ("feature_selection", fs),
    ("log_transformer", lt),
    ("pol", poly),
    ("scaler", scaler),
    ("model", reg)
])

poly_reg_pipe.fit(X_train, y_train)
print("Training Score")
print(round(poly_reg_pipe.score(X_train, y_train), 3))
print("Validation Score")
print(round(poly_reg_pipe.score(X_val, y_val), 3))
###week01_ex17a_end

###week01_ex17b_start_py
from sklearn.tree import DecisionTreeRegressor

reg = DecisionTreeRegressor(random_state=42)

reg_tree_pipe = Pipeline([
    ("drop_duplicated", DD),
    ("make_dataframe", DT),
    ("feature_selection", fs),
    ("transformer", lt),
    ("scaler", scaler),
    ("model", reg)
])

reg_tree_pipe.fit(X_train, y_train)
print("Training Score")
print(round(reg_tree_pipe.score(X_train, y_train), 3))
print("Validation Score")
print(round(reg_tree_pipe.score(X_val, y_val), 3))
###week01_ex17b_end

###week01_ex17c_start_md
Adding the polynomial features in improves model performance slightly on the training set but reduces performance on the validation set - it may be that the model is slightly "overfitting" to the data. 

The Decision Tree Regressor has the largest gap in performance between the training and validation data. Indeed it has perfect performance on the training set and bad performance on the validation set - a perfect example of overfitting (in later weeks we'll be able to examine why this might be).
###week01_ex17c_end

###week01_ex18_start_py
pipes_dict = {
    "Polynomial Features": poly_reg_pipe,
    "Regression Tree": reg_tree_pipe
}

for pipe_name in pipes_dict:
    print(pipe_name)
    scores = cross_validate(pipes_dict[pipe_name], X_train_full, y_train_full, cv=5, return_train_score=True)

    display(tidy_scores(scores))
###week01_ex18_switch_md
Assessing the models using cross-validation demonstrates that the pipeline with polynomial features appears to have better average performance on both the training and validation set. Furthermore, the difference between these two sets may not actually as bad as the previous train/validation split made out and could just be due to the specific split made. The difference in score for this pipeline between the training set and validation set is however still larger than the linear regression model, suggesting there is some overfitting occouring here. The Regression Tree is still consistently bad on default settings (we'll fix it in later weeks).
###week01_ex18_end

###week01_ex19_start_py
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.dummy import DummyClassifier

dd = duplicated_sampler
fs = DropCorrelatedFeatures(variables=None, 
                            method='pearson', 
                            threshold=0.6)
lt = LogTransformer()
scaler = StandardScaler() 

pre_processing = [
    ("drop_duplicated", dd),
    ("make_dataframe", DT),
    ("feature_selection", fs),
    ("log_transformer", lt),
    ("scaler", scaler)
]

dummy_pipe = Pipeline(steps=pre_processing+[("model", DummyClassifier(random_state=42))])
log_pipe = Pipeline(steps=pre_processing+[("model", LogisticRegression(random_state=42))])
svc_pipe = Pipeline(steps=pre_processing+[("model", SVC(random_state=42))])
knn_pipe = Pipeline(steps=pre_processing+[("model", KNeighborsClassifier())])

model_dict = {
    'Dummy': dummy_pipe,
    'Logistic Regression': log_pipe,
    'SVC': svc_pipe,
    'KNN': knn_pipe}

for i, model_name in enumerate(model_dict): 
    scores = cross_validate(model_dict[model_name], X_train_full, y_train_full, cv=5, return_train_score=True)
    scores_df = tidy_scores(scores)
    scores_df['model'] = model_name
    scores_df = scores_df.set_index("model", append=True)
    scores_df = scores_df.swaplevel()
    
    if i ==0:
        all_scores = scores_df
    else:
        all_scores = pd.concat([all_scores, scores_df], axis=0)

display(all_scores.xs('mean', level=1, drop_level=False))
display(all_scores.xs('sd', level=1, drop_level=False))
###week01_ex19_switch_md
It appears that the difference between the training and validation set is not to large for the dummy (expected) and logistic regression models, but is larger for the SVC and particularly the KNN model. Out of all the models, "SVC" appears to be the best.
###week01_ex19_end