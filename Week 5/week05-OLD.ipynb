{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-34f9557d-8f15-494e-8d65-74c8ae429c5c",
    "deepnote_cell_type": "markdown",
    "id": "gRJp4fGPvZ7c"
   },
   "source": [
    "# Week 5 - Linear regression\n",
    "by Colin Rundel & David Elliott & Kit Searle\n",
    "\n",
    "1. [Setup](#setup)\n",
    "\n",
    "2. [Least squares](#RBH)\n",
    "\n",
    "3. [Regression using scikit-Learn](#RSKL)\n",
    "\n",
    "4. [Regression with categorical cariables](#SKV)\n",
    "\n",
    "5. [Least Squares & rank deficiency ](#LSRD)\n",
    "\n",
    "6. [Multiple linear regression](#mlr)\n",
    "\n",
    "7. [Model refinement](#refine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AdHUSbWsvZ7h"
   },
   "source": [
    "This week we will be implementing the linear regression techniques from this weeks lecture in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-645a25eb-6010-425a-88c0-ecf0093a9edc",
    "deepnote_cell_type": "markdown",
    "id": "6sVlUI4SvZ7i"
   },
   "source": [
    "---\n",
    "\n",
    "# 1. Setup <a id='setup'></a>\n",
    "\n",
    "## 1.1. Uploading your data to the colab\n",
    "This notebook will be saved in your google drive in a folder \"Colab Notebooks\" by default, you should be fairly familiar with this by now.\n",
    "\n",
    "When you run this cell you will need to give colab permission to access files in your drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ZeTuzK-dvZ7j"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "os.chdir('drive/My Drive/Colab Notebooks/mlp/week-5')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trxL3QeSvZ7k"
   },
   "source": [
    "We will now unzip the workshop materials and place them in a subdirectory \"ws-material/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "5DbL_UYJvZ7k"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('ws-material'):\n",
    "  print('Unzipping materials...')\n",
    "  !unzip week-05.zip -d ws-material\n",
    "else:\n",
    "  print(\"Directory already exists!\")\n",
    "\n",
    "os.chdir('ws-material')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c8FHFpfevZ7l"
   },
   "source": [
    "__Reminder__ \n",
    "\n",
    "- You may need to restart the runtime several times in the workshop, but you will not need to re-upload or unzip files again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEIja_2pvZ7n"
   },
   "source": [
    "## 1.2 Packages\n",
    "\n",
    "Now lets load in the packages you wil need for this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-d0af5d8f-8894-4c5a-b754-353993666790",
    "collapsed": true,
    "deepnote_cell_type": "code",
    "id": "KBfh_AXdvZ7o",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "# Display plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting defaults\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "\n",
    "# sklearn modules\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mC7t8FfQvZ7p"
   },
   "source": [
    "###  Helper Functions\n",
    "\n",
    "Below are two helper functions we will be using in this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "biPA_OxTvZ7p"
   },
   "outputs": [],
   "source": [
    "def get_coefs(m):\n",
    "    \"\"\"Returns the model coefficients from a Scikit-learn model object as an array,\n",
    "    includes the intercept if available.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If pipeline, use the last step as the model\n",
    "    if (isinstance(m, sklearn.pipeline.Pipeline)):\n",
    "        m = m.steps[-1][1]\n",
    "    \n",
    "    \n",
    "    if m.intercept_ is None:\n",
    "        return m.coef_\n",
    "    \n",
    "    return np.concatenate([[m.intercept_], m.coef_])\n",
    "\n",
    "def model_fit(m, X, y, plot = False):\n",
    "    \"\"\"Returns the root mean squared error of a fitted model based on provided X and y values.\n",
    "    \n",
    "    Args:\n",
    "        m: sklearn model object\n",
    "        X: model matrix to use for prediction\n",
    "        y: outcome vector to use to calculating rmse and residuals\n",
    "        plot: boolean value, should fit plots be shown \n",
    "    \"\"\"\n",
    "    \n",
    "    y_hat = m.predict(X)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_hat))\n",
    "    \n",
    "    res = pd.DataFrame(\n",
    "        data = {'y': y, 'y_hat': y_hat, 'resid': y - y_hat}\n",
    "    )\n",
    "    \n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(121)\n",
    "        sns.lineplot(x='y', y='y_hat', color=\"grey\", data =  pd.DataFrame(data={'y': [min(y),max(y)], 'y_hat': [min(y),max(y)]}))\n",
    "        sns.scatterplot(x='y', y='y_hat', data=res).set_title(\"Fit plot\")\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        sns.scatterplot(x='y', y='resid', data=res).set_title(\"Residual plot\")\n",
    "        plt.hlines(y=0, xmin=np.min(y), xmax=np.max(y), linestyles='dashed', alpha=0.3, colors=\"black\")\n",
    "        \n",
    "        plt.subplots_adjust(left=0.0)\n",
    "        \n",
    "        plt.suptitle(\"Model rmse = \" + str(round(rmse, 4)), fontsize=16)\n",
    "        plt.show()\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-90709695-8746-4669-9199-fd144a6ec872",
    "deepnote_cell_type": "markdown",
    "id": "yz3bjxcbvZ7r"
   },
   "source": [
    "## 1.3 Data\n",
    "\n",
    "To begin, we will examine a simple data set on the medical costs which comes from the `Medical Cost Personal` data set. Our goal is to model the yearly medical charges of an individual using some combination of the other features in the data. The included columns are as follows:\n",
    "\n",
    "* `charges` - yearly medical charges in USD\n",
    "* `age` - the individuals age\n",
    "* `sex` - the individuals sex, either `\"male\"` or `\"female\"`\n",
    "* `bmi` - the body mass index of the individual\n",
    "* `children` - the number of dependent children the individual has\n",
    "* `smoker` - a factor with levels `\"yes\"`, the individual is a smoker and `\"no\"`, the individual is not a smoker\n",
    "\n",
    "We read the data into python using pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eF19U6ivvZ7r"
   },
   "outputs": [],
   "source": [
    "df_insurance = pd.read_csv(\"./Data/insurance.csv\")\n",
    "df_insurance[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izo4A3SSvZ7t"
   },
   "source": [
    "We will begin by constructing a pairs plot of our data and examining the relationships between our variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-da6d5f4b-bf6b-43f7-9751-a015e6a9924e",
    "deepnote_cell_type": "markdown",
    "id": "50AbKP76vZ7u"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 1\n",
    "\n",
    "Create a pairs plot of these data (make sure to include the `smoker` column), describe any relationships you observe in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-4d9907ea-c7c7-4c16-aee7-5e12c3e00cc1",
    "deepnote_cell_type": "code",
    "id": "-mG-rvtrvZ7u",
    "output_cleared": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-a3d1594c-000b-48dd-b347-dfa5410fe0a7",
    "deepnote_cell_type": "markdown",
    "id": "01HS3aS8vZ7u"
   },
   "source": [
    "---\n",
    "\n",
    "Add your text solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xhjuXdcvZ7v"
   },
   "source": [
    "### 1.4.1 Test Set\n",
    "We will split our data into two distinct sets. The first called the `training set` will contain our test data `X_train` and `y_train` which is used for determining the feature weights. The second called the `test set` will contain our testing data `X_test` and `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ox4hiGT3vZ7v"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df_insurance.drop('charges',axis=1) # Independet variable\n",
    "y = df_insurance['charges'] # dependent variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-7d871b29-2a08-4b7f-8421-d36b23056000",
    "deepnote_cell_type": "markdown",
    "id": "f8cnwbvKvZ7v"
   },
   "source": [
    "# 2. Least Squares  <a id='RBH'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-ca8096af-6bc2-4bf1-aea6-19d55952f208",
    "deepnote_cell_type": "markdown",
    "id": "wyQLrGCvvZ7w"
   },
   "source": [
    "We will begin by fitting a simple linear regression model for `charges` exclusively using `bmi` as a feature in our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-92e05d0b-dedd-444b-a9f3-df423c4b0e70",
    "deepnote_cell_type": "markdown",
    "id": "JdkqCIkUvZ7w"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 2\n",
    "\n",
    "Create a scatter plot using the `insurance_df` data frame describe any apparent relationship between `charges` and `bmi` (make sure to include the smoker `column`).\n",
    "\n",
    "Hint: You can use the input hue =\"something\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00011-abd9df3c-dd0b-43bc-a4b4-4369e7a9cc3e",
    "deepnote_cell_type": "code",
    "id": "UJt4fEW2vZ7w",
    "output_cleared": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-fbc0c5b1-65c6-41e3-b85a-6000792a1342",
    "deepnote_cell_type": "markdown",
    "id": "auQQhAL3vZ7x"
   },
   "source": [
    "In lecture we discussed how we can represent a regression problem using matrix notation and we can derive a solution using least squares. We can express this as,\n",
    "\n",
    "$$\n",
    "\\underset{\\boldsymbol{\\beta}}{\\text{argmin}} \\,\\, \\lVert \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta} \\rVert^2 \n",
    "= \\underset{\\boldsymbol{\\beta}}{\\text{argmin}} \\,\\, (\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta})^\\top(\\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta})\n",
    "$$\n",
    "\n",
    "where,\n",
    "\n",
    "$$\n",
    "\\underset{n \\times 1}{\\boldsymbol{y}} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n-1} \\\\ y_n \\end{pmatrix} \n",
    "\\qquad\n",
    "\\underset{n \\times 2}{\\boldsymbol{X}} = \\begin{pmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_{n-1} \\\\ 1 & x_n \\\\ \\end{pmatrix}\n",
    "\\qquad \n",
    "\\underset{2 \\times 1}{\\boldsymbol{\\beta}} = \\begin{pmatrix} \\beta_0 \\\\ \\beta_1 \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-e55877d4-ea16-4e4a-8b06-c452fee69c1e",
    "deepnote_cell_type": "markdown",
    "id": "UonIJIH4vZ7y"
   },
   "source": [
    "The solution to this optimization problem is,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\beta} = \\left(\\boldsymbol{X}^\\top\\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^\\top\\boldsymbol{y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-0a1e2d21-4ae7-45f0-b0d5-da7f9fc56067",
    "deepnote_cell_type": "markdown",
    "id": "FOuGVr9YvZ7y"
   },
   "source": [
    "In Python we can construct the model matrix `X` by combining a column of ones, for the intercept, with our observed `bmi` values. Similarly, `y` is a column vector of the `charges` values. In both cases we construct these objects as numpy array objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00017-21e29cd3-56a0-4b48-899b-4409e4b6f9e2",
    "deepnote_cell_type": "code",
    "id": "12qcB6mNvZ7y",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "y = np.array(y_train)\n",
    "print(y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00018-7d43b31a-6d5b-481f-9f2f-172e204c6c9a",
    "deepnote_cell_type": "code",
    "id": "X-4i8oFqvZ7y",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "X = np.c_[\n",
    "    np.ones(len(y_train)),\n",
    "    X_train.bmi\n",
    "]\n",
    "\n",
    "print(X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00019-17b55704-7eca-4662-a6a9-7de93e566521",
    "deepnote_cell_type": "markdown",
    "id": "O0cIiKSsvZ7z"
   },
   "source": [
    "Given the model matrix $(\\boldsymbol{X})$ and observed outcomes $(\\boldsymbol{y})$ we can then calculate the vector of solutions $(\\boldsymbol{\\beta})$ using numpy,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00020-f44bb993-8049-4e28-8da0-c8a2419a4950",
    "deepnote_cell_type": "code",
    "id": "11hAhtoHvZ7z",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import solve\n",
    "beta = solve(X.T @ X, X.T @ y)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00021-611b22a9-a494-4ab7-a88a-ceed7c58b0a1",
    "deepnote_cell_type": "markdown",
    "id": "SEqT8F7fvZ7z"
   },
   "source": [
    "Note that when using numpy `@` performs  matrix multiplication while `*` performs elementwise multiplication between arrays. Numpy matrix multiplication can also be written using `A.dot(B)` or `np.matmul(A,B)`.\n",
    "\n",
    "We can calculate predictions from this model by calculating $\\hat{y} = \\boldsymbol{X} \\boldsymbol{\\beta}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00022-28a7a9d1-e95b-47b7-a31c-9b247c628087",
    "deepnote_cell_type": "markdown",
    "id": "mvrOq4afvZ7z"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 3\n",
    "\n",
    "Calculate these predicted charges and store them in the origin `X_train` data frame in a column called `charges_pred`. Print out the updated version of the data frame with this new column added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00023-04722d32-f420-4104-b25b-37656a71df76",
    "deepnote_cell_type": "code",
    "id": "3Mxs7ORhvZ71",
    "output_cleared": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00024-66ad5764-457d-4ffe-9f28-cd42b9a0b5b7",
    "deepnote_cell_type": "markdown",
    "id": "kM4RGV_evZ72"
   },
   "source": [
    "---\n",
    "\n",
    "Given the predictions we can create a plot showing the models fit by overlaying a line plot of the predictions on top of the original scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00025-67f4357b-0a2c-46a2-90f1-8a75e8b62e2f",
    "deepnote_cell_type": "code",
    "id": "xuICvR2OvZ72",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "X_train[\"actual\"] = y_train\n",
    "sns.scatterplot(x=\"bmi\", y=\"actual\",hue = \"smoker\", data=X_train)\n",
    "sns.lineplot(x=\"bmi\", y=\"charges_pred\", color=\"black\", data=X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uoEYNyw0Jfm"
   },
   "source": [
    "## 2.1 Residuals\n",
    "\n",
    "From the figure above, do you think that this model is a good fit? \n",
    "\n",
    "One of the most useful tools for evaluating a model is to examine the residuals of that model. For any standard regression model the residual for observation $i$ is defined as $y_i - \\hat{y}_i$ where $\\hat{y}_i$ is the model's predicted value for observation $i$. As mentioned previous, for the case of linear regression\n",
    "$\\hat{y} = \\boldsymbol{X} \\boldsymbol{\\beta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Szq6fbGf05A9"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 4\n",
    "\n",
    "Calculate the residual for each observation and store them in the origin `X_train` data frame in a column called named `resid`. Using this new column create a residual plot (scatter plot of volume vs resid) for this model. Color the points based on weather the individual is a smoker or not.\n",
    "\n",
    "Hint: You can either use `y_train` or use the collumn `actual` in `X_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRdjYEgP1ahd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIICk-3l4TVj"
   },
   "source": [
    "What do you notice about this plot? Hold that thought, we will get back to this later.\n",
    "\n",
    "The $R^2$ (also called the coefficient of determination) statistic provides an alternative measure of evaluating the usefullness of the model. It takes the form of a proportion, the proportion of variance explained by the independent variable. To calculate the $R^2$ we use the formula $$R^2 = 1- \\frac{RSS}{TSS},$$ where $$RSS = \\sum_{i=1}^{N}(y_i -\\hat{y}_i)^2$$ is called the residual sum of squares, $$TSS = \\sum_{i=1}^{N}(y_i -\\overline{y})^2$$ is called the total sum of squares and $\\overline{y}$ is the sample mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7foBrtx5DXO"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 5\n",
    "\n",
    "Calculate the $R^2$ score for this model and comment on the value which you obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClWZ7isN5ekk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqZpAT1MxolL"
   },
   "source": [
    "---\n",
    "\n",
    "Add your text solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00026-021075dd-c202-41db-9f49-2b4de4efd791",
    "deepnote_cell_type": "markdown",
    "id": "ilhDv3WHvZ73"
   },
   "source": [
    "---\n",
    "# 3 Regression using scikit-Learn <a id='RSKL'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00027-068e26c7-da60-4c7c-8551-5f914b53af75",
    "deepnote_cell_type": "markdown",
    "id": "NtUQzMD9vZ73"
   },
   "source": [
    "Constructing the model matrix by hand and calculating $\\boldsymbol{\\beta}$ and model predictions using the least squares solution is less than ideal. As you might expect there are a number of higher level libraries that take care of many of these details. In this course we will be using the **scikit-learn** (**sklearn**) library to implement most of our machine learning models. As the semester progresses we will be learning about and implementing many different modeling methods. Additionally, we will also be learning how to use the larger data processing and workflow tools that are available in this library.\n",
    "\n",
    "sklearn separates its various modeling tools into submodules organized by model type - for today we will be using the `LinearRegression` model from the `linear_model` submodule. Which we can import as follows,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00028-40915b00-d692-4d82-9df3-1a72901e798d",
    "collapsed": true,
    "deepnote_cell_type": "code",
    "id": "WBf8UcpzvZ73",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00029-a3ead172-d8aa-4570-9aae-68cd6d18692d",
    "deepnote_cell_type": "markdown",
    "id": "o2W30cCUvZ73"
   },
   "source": [
    "In general sklearn's models are implemented by first creating a model object, which is configured via constructor arguments, and then using that object to fit your data. As such, we will now create a linear regression model object `lr` and use it to fit our data. Once this object is created we use the `fit` method to obtain a model object fitted to our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00030-d0c72235-001d-4cd4-a50f-302b40d9e4d3",
    "collapsed": true,
    "deepnote_cell_type": "code",
    "id": "VcuBnGPavZ73",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "l = lr.fit(\n",
    "    # X must be a matrix so we need to reshape the column\n",
    "    X = np.array(X_train.bmi).reshape(-1,1), \n",
    "    y = y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00031-b1d09da5-a8c6-46db-a5ec-617662e344a9",
    "deepnote_cell_type": "markdown",
    "id": "kMZfWvLKvZ74"
   },
   "source": [
    "This model object then has various useful methods and attributes, including `intercept_` and `coef_` which contain our estimates for $\\boldsymbol{\\beta}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00032-d4cd3759-a588-4d95-b34e-507416ebc046",
    "deepnote_cell_type": "code",
    "id": "Rt0cuv8uvZ74",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "b0 = l.intercept_\n",
    "b1 = l.coef_[0]   # Subsetting here returns a scalar value\n",
    "beta = (b0, b1)\n",
    "\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00033-9aecc749-af3c-4332-a238-370b1c7febc5",
    "deepnote_cell_type": "markdown",
    "id": "jnhiFSQfvZ74"
   },
   "source": [
    "Using this default construction of `LinearRegression`, sklearn assumes that we have not included an intercept column (ones) in our model matrix and takes care of this for you. Additionally, since the intercept column is added the $\\beta$ estimated for this particular column is stored separately, in the `intercept_` attribue.\n",
    "\n",
    "I generally find this default behavior to be somewhat frustrating to work with, instead my preference is to handle all of the details of constructing the model matrix `X` myself and retrieving all `beta` values (including the intercept) from `coef_` directly. For example, if we use the `X` and `y` variables we defined for the least squares example above and construct the `LinearRegression` object using `fit_intercept=False` then,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00034-f6c94c6a-6fd5-4963-86db-3a815ae09eda",
    "deepnote_cell_type": "code",
    "id": "da-pgorbvZ75",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "l = LinearRegression(fit_intercept=False).fit(X = X, y = y)\n",
    "beta = l.coef_\n",
    "print(beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00035-9bb64eec-8879-48d5-832a-101c7a3dc752",
    "deepnote_cell_type": "markdown",
    "id": "lLUKEr2YvZ75"
   },
   "source": [
    "Note that this is the same answers we obtained above.\n",
    "\n",
    "The model fit objects also provide additional useful methods for evaluating the model $R^2$ (`score`) and calculating predictions (`predict`). Using the later we can add another column of predictions to our data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00036-fdd889c3-0b7b-449e-a815-6e1e055f9703",
    "collapsed": true,
    "deepnote_cell_type": "code",
    "id": "rqv-rPpQvZ75",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "X_train[\"charges_skl_pred\"] = l.predict(X)\n",
    "l.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00037-fcdee620-68f8-4497-80c8-de602dd5eb66",
    "deepnote_cell_type": "markdown",
    "id": "Je3tzgaNvZ75"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise *6*\n",
    "\n",
    "Do these results agree with the results we obtained when using the numpy least squares method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EW10NyWz8vK-"
   },
   "source": [
    "---\n",
    "\n",
    "Add your text solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00045-2510df6f-73e5-4a95-a0e6-84c3fcfc0d85",
    "deepnote_cell_type": "markdown",
    "id": "FejpEQrxvZ77"
   },
   "source": [
    "---\n",
    "\n",
    "## 4. Regression with categorical cariables <a id='RCV'></a>\n",
    "\n",
    "At the end of exersise 2, you should have noticed that most of the smokers had a positive residual while the non smokers have a negative residule. Based on these results, it should be clear that it is important that our model include information about whether or not the individual is a smoker or not. As such, we need a way of encoding this information into our modeling framework. To do this we need a way of converting our string / categorical variable into a numeric representation that can be included in our model matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00046-c8a96bcf-e22c-455d-b8f7-38de9f5226d0",
    "deepnote_cell_type": "markdown",
    "id": "G7JYVzIfvZ77"
   },
   "source": [
    "### 4.1 Dummy Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00047-468c75ad-fcb3-4887-a165-c6d54e30a250",
    "deepnote_cell_type": "markdown",
    "id": "jQhY613rvZ78"
   },
   "source": [
    "\n",
    "The most common approach for doing this is called dummy coding, in the case of a binary categorical variable it involves picking one of the two levels of the categorical variable and encoding it as 1 and the other level as 0. With Python we can accomplish this by comparing our categorical vector to the value of our choice and then casting (converting) the result to an integer type.\n",
    "\n",
    "For example if we wanted to code `smoker` as 1 and `nonsmoker` as 0 we would do the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00048-e0a6938a-2785-45be-8495-613bd782b146",
    "deepnote_cell_type": "code",
    "id": "ZAyd-mOUvZ78",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "X_train[\"smoker_yes\"] = (X_train.smoker == \"yes\").astype(int) # Returns either 0 or 1\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00049-4d15a469-b618-486d-bccd-871ee7b40a5e",
    "deepnote_cell_type": "markdown",
    "id": "YW0RAW9UvZ78"
   },
   "source": [
    "This is equivalent to using an indicator function in mathematical notation,\n",
    "\n",
    "$$ \n",
    "\\mathbb{1}_{s_i} = \n",
    "\\begin{cases}\n",
    "1 & \\text {if individual $i$ is a smoker} \\\\\n",
    "0 & \\text {if individual $i$ is not a smoker}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Alternatively, we can defined the opposite of this where we code `smoker` as 0 and `nonsmoker` as 1,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00050-23300ecd-113d-41d8-a559-3a5de5616782",
    "deepnote_cell_type": "code",
    "id": "XBt9J9UzvZ79",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "X_train[\"smoker_no\"] = (X_train.smoker == \"no\").astype(int) # Returns either 0 or 1\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00051-ae64579f-1cea-4e7e-a044-10f7a2aee013",
    "deepnote_cell_type": "markdown",
    "id": "GEG5-NAQvZ79"
   },
   "source": [
    "Now that we have recoded our categorical variable, `smoker`, into a numerical variable we can fit a standard regression model with the form,\n",
    "\n",
    "$$ y_i = \\beta_0 + \\beta_1 \\, x_i + \\beta_2 \\, \\mathbb{1}_{s_i} $$\n",
    "\n",
    "which we can represent in matrix form using, $\\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta}$\n",
    "where $\\boldsymbol{X} = \\big[ \\boldsymbol{1},\\, \\boldsymbol{x},\\, \\boldsymbol{\\mathbb{1}_{s}} \\big]$.\n",
    "\n",
    "Using Python, we can use the concatenate function with our 1s column, the `bmi` column, and our new dummy coded indicator column, `smoker_yes`,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00052-a2d2f82b-236a-4285-bb0c-c8477e3ae8df",
    "deepnote_cell_type": "code",
    "id": "xdCXgKk5vZ79",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "X = np.c_[np.ones(len(y_train)), \n",
    "          X_train.bmi, \n",
    "          X_train.smoker_yes]\n",
    "l = LinearRegression(fit_intercept=False).fit(X, y_train)\n",
    "\n",
    "beta = l.coef_\n",
    "\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LVIbCPTSndI"
   },
   "source": [
    "This gives us a regression equation of the form,\n",
    "\n",
    "$$ \n",
    "y_i = -3622.02 + 393.71 \\, x_i + 24026.45 \\, \\mathbb{1}_{s_i} \n",
    "$$\n",
    "\n",
    "which can be rewritten as two separate line equations (one for each case of `smoker`),\n",
    "\n",
    "$$\n",
    "y_i = \\begin{cases}\n",
    "        -3622.02 + 393.71 \\, x_i & \\text{if individual $i$ is a not smoker} \\\\\n",
    "        (-3622.02 + 24026.45) + 393.71 \\, x_i & \\text{if individual $i$ is a smoker.} \\\\\n",
    "       \\end{cases}\n",
    "$$\n",
    "\n",
    "We can calculate prediction points along those lines using the following Python code in which we hard code the possible values of $\\boldsymbol{\\mathbb{1}_{s_i}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00054-0ddac0c5-1574-49ce-9363-aac34646813b",
    "deepnote_cell_type": "code",
    "id": "FjzEMUmBvZ7-",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "X_train[\"charges_smoker_pred\"] = l.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00055-91fdbf58-e6a6-40de-b90d-0732d78cf919",
    "deepnote_cell_type": "markdown",
    "id": "GIAZDvZSvZ7_"
   },
   "source": [
    "and we can then plot both of these lines along with the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00056-93efff57-d781-4351-9b19-08e434e15f57",
    "deepnote_cell_type": "code",
    "id": "jWrdQ5dzvZ7_",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"bmi\", y=\"actual\", hue=\"smoker\", data=X_train)\n",
    "sns.lineplot(x=\"bmi\", y=\"charges_smoker_pred\", hue=\"smoker\", data=X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00057-d749fb2e-f46e-4e32-820b-0e528a5d19c5",
    "deepnote_cell_type": "markdown",
    "id": "mXQVoCexvZ7_"
   },
   "source": [
    "As well as create plot a residual plot of this new model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00058-c0a91d06-1802-44dd-b1fa-24788c4c9125",
    "deepnote_cell_type": "code",
    "id": "GDMvyck6vZ7_",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "X_train[\"resid_smoker\"] = X_train.actual - X_train.charges_smoker_pred\n",
    "sns.scatterplot(x=\"actual\", y=\"resid_smoker\", hue=\"smoker\", data=X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00059-fef18a7d-f736-4ae5-9bc0-94cceaae2b1e",
    "deepnote_cell_type": "markdown",
    "id": "YRqSSQXvvZ8A"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 7\n",
    "\n",
    "Based on these regression fits, do you think the model including the dummy coded `smoker` variable produces a \"better\" model than our first regression model which did not include `smoker`? Explain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00060-05c7e049-ccbf-43bb-bd35-e949d9705323",
    "deepnote_cell_type": "markdown",
    "id": "T-cLQHcwvZ8A"
   },
   "source": [
    "---\n",
    "\n",
    "Add your text solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00061-0359dd99-1936-4981-906a-149556d2e6cf",
    "deepnote_cell_type": "markdown",
    "id": "YjevZufIvZ8A"
   },
   "source": [
    "---\n",
    "\n",
    "Note that by including a dummy variable in our model will change the interpretation of our regression coefficients. In this context,\n",
    "\n",
    "* $\\beta_0$ - This is the expected charges for in individual with a `bmi` of zero and a `smoker` indicator of zero, in other words a nonsmoker with zero bmi.\n",
    "\n",
    "* $\\beta_1$ - This is the expected additional charges an individual would incure if their bmi were to increase by 1 unit, all else being equal.\n",
    "\n",
    "* $\\beta_2$ - This is the expected additional charges an individual would incure if their indicator were to increase by 1, all else being equal. However, the smoker indicator can only be 0 or 1 and hence this is the change in charges we would expect between a smoker and a nonsmoker with the same bmi. In other words, smokers should expect to pay $24026 more for healthcare than nonsmokers.\n",
    "\n",
    "Based on these interpretations we can see that the level that was coded as 0 (what is often called the reference level) gets folded into our intercept and the slope coefficient for the indicator provides the difference in intercept between the reference and the contrast level (level coded as 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00066-fe6afd8c-708d-452c-9da5-82776f301e98",
    "deepnote_cell_type": "markdown",
    "id": "53oIHUh2vZ8B"
   },
   "source": [
    "---\n",
    "\n",
    "### 4.2 One hot encoding\n",
    "\n",
    "Another common approach for transforming categorical variables is know as one hot encoding, in which all levels of the categorical variable are transformed into a new columns with values of 0 or 1. This is equivalent to what we have done manually above by including both `smoker_yes` and `smoker_no`. This differs from dummy coding in that there is no longer a reference factor.\n",
    "\n",
    "Pandas has a built-in method for performing this on categorical columns. This is easiest to see with a simple example, below we construct a data frame `df` with a single column that we transform into a one hot encoded version using panda's `get_dummies` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00067-c569ae1f-50a5-48e7-b52c-9202ff2460a8",
    "collapsed": true,
    "deepnote_cell_type": "code",
    "id": "11ECvfs4vZ8B",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "X_train_dummy = pd.get_dummies(X_train[[\"bmi\", \"smoker\"]], columns=[\"smoker\"])\n",
    "X_train_dummy[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00075-57bb54b9-c865-4ca9-8da7-9a518891e2ba",
    "deepnote_cell_type": "markdown",
    "id": "dqQnSH5evZ8D"
   },
   "source": [
    "Note that `get_dummies` does not modify the underlying dataframe in place, and that it is necessary to save the result to a new variable (or overwrite the old version)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00076-d00291ff-97cc-4a4a-b877-db2c8fea0050",
    "deepnote_cell_type": "markdown",
    "id": "E8spetoavZ8D"
   },
   "source": [
    "---\n",
    "\n",
    "# 5 Least Squares & rank deficiency <a id='LSRD'></a>\n",
    "\n",
    "Now lets consider the model where we naively include both `smoker_yes` and `smoker_no` as well as an intercept column in our model matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00077-9f26669e-cc42-448a-ae1d-ce4ee2f07591",
    "collapsed": true,
    "deepnote_cell_type": "code",
    "id": "onHoyinNvZ8D",
    "output_cleared": true
   },
   "outputs": [],
   "source": [
    "X = np.c_[\n",
    "    np.ones(len(y_train)), \n",
    "    X_train.bmi, \n",
    "    X_train.smoker_yes,\n",
    "    X_train.smoker_no\n",
    "]\n",
    "l = LinearRegression(fit_intercept=False).fit(X, y_train)\n",
    "\n",
    "\n",
    "beta = l.coef_\n",
    "print( beta )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00078-37df9229-80c6-4d30-8452-ad168b04e9c9",
    "deepnote_cell_type": "markdown",
    "id": "JcULYMG_vZ8D"
   },
   "source": [
    "This gives us a regression equation of the form,\n",
    "\n",
    "$$\n",
    "y_i = \\begin{cases}\n",
    "        5594.14 + 393.71 \\, x_i + 14810.29(0) - 9216.16(1) \\\\= -3622.02 + 393.71 \\, x_i & \\text{if individual $i$ is not a smoker} \\\\\n",
    "        5594.14 + 393.71 \\, x_i + 14810.29(1) - 9216.16(0) \\\\= 20407.43 + 0.6 \\, x_i & \\text{if individual $i$ is a smoker.} \\\\\n",
    "       \\end{cases}\n",
    "$$\n",
    "\n",
    "However, the solutions ($\\boldsymbol{\\beta}$) are not unique, anything of the form $\\beta_0 + \\beta_2 = a$ and $\\beta_0 + \\beta_3 = b$ will work. In other words we have colinearity between our predictors - if you examine the data it should be clear that given any two of the intercept, `svi_0`, and `svi_1` it is possible to exactly determine the value of the other column. Mathematically, we describe this as these columns are linearly depenedent, which implies that our model matrix is *rank deficient*. You can check this explicitly by via the `numpy.linalg.matrix_rank` function which will report that `X` (and $\\boldsymbol{X}^\\top\\boldsymbol{X}$) are of rank 3 not 4 which is what we might have naievely expected.\n",
    "\n",
    "This is important as the underlying linear algrebra methods used to solve for $\\beta$ for a least squares problem often implicitly assume that $\\boldsymbol{X}^\\top\\boldsymbol{X}$ is full rank in order to solve the matrix inverse and violating these assumptions can have unexpected results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oC3EJhMv2UWX"
   },
   "source": [
    "---\n",
    "\n",
    "# 6 Multiple linear regression <a id='MLR'></a>\n",
    "\n",
    "Now we understand the basics of the linear regression, lets fit a baseline model with all the features to use as a point of comparison for our subsequent models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7F6ERMfV2ixu"
   },
   "outputs": [],
   "source": [
    "full_dummies = pd.get_dummies(df_insurance[[\"charges\",\"bmi\", \"age\", \"children\",\"smoker\", \"sex\"]], columns=[\"sex\", \"smoker\"])\n",
    "\n",
    "X = full_dummies.drop('charges',axis=1) # Independet variables\n",
    "y = full_dummies['charges'] # dependent variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=23)\n",
    "\n",
    "X = np.c_[X_train.bmi, \n",
    "          X_train.age,\n",
    "          X_train.children,\n",
    "          X_train.sex_female,\n",
    "          X_train.smoker_yes\n",
    "          ]\n",
    "\n",
    "lm = LinearRegression().fit(X, y_train)\n",
    "beta = get_coefs(lm)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoOQeUu26rSk"
   },
   "source": [
    "These coefficients have the typical regression interpretation, e.g. for each unit increase in `bmi` we expect the `charges` to increase by 334.12 on average. These values are not of particular interest for us for this particular problem as we are more interested in the predictive properties of our model(s). \n",
    "\n",
    "Note that in this instance we have not created the intercept column but rather allowed sklearn to handle this.\n",
    "\n",
    "To evaluate this we will use the model_fit helper function defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZSxm_Nq63jL"
   },
   "outputs": [],
   "source": [
    "X_test_matrix = np.c_[X_test.bmi, \n",
    "          X_test.age,\n",
    "          X_test.children,\n",
    "          X_test.sex_female,\n",
    "          X_test.smoker_yes\n",
    "          ]\n",
    "\n",
    "model_fit(lm, X_test_matrix, y_test, plot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UY-EpvtvA6MJ"
   },
   "source": [
    "Primarily we will use this function to obtain the rmse of our model using the test data (`X_test` and `y_test`). Note that we fit the model using the training data (`X_train` and `y_train`). We have also included a fit ($y$ vs $\\hat{y}$) and resid ($y$ vs $y-\\hat{y}$) plot of these results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRvJ0BQ-BMZG"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 8\n",
    "\n",
    "Would you expect the rmse of the model to be better or worse when using the training data (compared to the validation data)? Check your answer using the `model_fit` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lKzNd3fBSz1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNaHNdMbBckF"
   },
   "source": [
    "---\n",
    "\n",
    "Add your text solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGD3tayKB1T-"
   },
   "source": [
    "---\n",
    "\n",
    "### 5.1 Standardization\n",
    "\n",
    "In subsequent sections we will be exploring the use of the Ridge and Lasso regression models which both penalize larger values of $\\boldsymbol{\\beta}$. While not particularly bad, our baseline model had $\\boldsymbol{\\beta}$s that ranged from the smallest at -12325 to the largest at 24263.87678352 which is about a 2x difference in magnitude.\n",
    "\n",
    "To deal with this issue, the standard approach is to center and scale *all* features to a common scale before fitting one of these models. The typical scaling approach is to subtract the mean of each feature and then divide by its standard deviation - this results in all feature columns having a mean of 0 and a variance of 1. Additionally, the feature values can now be interpreted as the number of standard deviations each observation is away from that column's mean.\n",
    "\n",
    "Using sklearn we can perform this transformation using the `StandardScaler` transformer from the `preprocessing` submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgtO-VlFDFQ-"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "S = StandardScaler().fit(full_dummies.drop(['charges',\"sex_male\", \"smoker_no\"], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nN3c86BDFBY"
   },
   "source": [
    "Once fit, we can examine the values used for the scaling by checking the mean_ and var_ attributes of the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MY1JXCFHDlDl"
   },
   "outputs": [],
   "source": [
    "S.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qI68AgpADnax"
   },
   "outputs": [],
   "source": [
    "S.var_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4KkFv3-EXai"
   },
   "source": [
    "Keep in mind, that the training, testing, and validation sets will not necessarily have the same feature column means and standard deviations - as such it is important that we choose a consistent set of values that are used for all of the data. In other words, be careful to not expect that StandardScaler().fit_transform(X_train) and StandardScaler().fit(X).transform(X_train) will give the same answer. The best way to avoid this issue is to include the StandardScaler in a modeling pipeline for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_07pDLizGiQC"
   },
   "outputs": [],
   "source": [
    "lm_scaled = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LinearRegression()\n",
    ").fit(X, y_train)\n",
    "\n",
    "\n",
    "get_coefs(lm_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWNsiOJ1HxVL"
   },
   "outputs": [],
   "source": [
    "model_fit(lm_scaled, X_test_matrix, y_test, plot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBW89Q4OHxDu"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 9\n",
    "\n",
    "Using this new model what has changed about our model results? Comment on both the model's coefficients as well as its predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxzljIQXJbGe"
   },
   "source": [
    "---\n",
    "\n",
    "Add your text solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GzaDyY-vJqah"
   },
   "source": [
    "--- \n",
    "\n",
    "## 5.2 Ridge Regression\n",
    "\n",
    "Ridge regression is a natural extension to linear regression which introduces an $\\ell_2$ penalty on the coefficients to a standard least squares problem. Mathematically, we can express this as the following optimization problem,\n",
    "\n",
    "$$ \\underset{\\boldsymbol{\\beta}}{\\text{argmin}} \\; \\lVert \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta} \\rVert^2_2 + \\alpha (\\boldsymbol{\\beta}^T\\boldsymbol{\\beta}) $$ \n",
    "\n",
    "The `Ridge` model is provided by the `linear_model` submodule and requires a single parameter `alpha` which determines the tuning parameter that adjusts the weight of the $\\ell_2$ penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVArS4IrJz8E"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "r = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    Ridge(alpha=20)\n",
    ").fit(X, y_train)\n",
    "print(get_coefs(r))\n",
    "\n",
    "model_fit(r, X_test_matrix, y_test, plot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gf9yq-uLxbu"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 10\n",
    "\n",
    "Adjust the value of alpha in the cell above and rerun it. Qualitatively, how does the model fit change as alpha changes? How does the rmse change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yB6vb-R5L527"
   },
   "source": [
    "---\n",
    "\n",
    "Add your text solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIUZtjLuMIJU"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 11\n",
    "\n",
    "In Section 5.4 we mentioned the importance of scaling features before fitting a Ridge regression model. The code below fits the Ridge model to the untransformed training data - repeat Exercise 10 using these data. How does the model fit change as alpha changes? How does the rmse change? How does the models performance compare to the previous model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ffjs7BFBMYgk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ok56-4xJNGRH"
   },
   "source": [
    "---\n",
    "\n",
    "Add your text solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sd_Mqyg0No7j"
   },
   "source": [
    "---\n",
    "\n",
    "### 5.2.1. Ridge $\\beta$s as a function of $\\alpha$\n",
    " \n",
    "Finally, one of the useful ways of thinking about the behavior of Ridge regression models is to examine the relationship between our choice of $\\alpha$ and the resulting $\\beta$s relative to the results we would have obtained from the linear regression model. Since Ridge regression is equivalent to linear regression when $\\alpha=0$ we can see that as we increase the value of $\\alpha$ we are shrinking all of the $\\beta$s towards 0 asymptotically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGORm3NHNoe9"
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-2, 3, num=200) # from 10^-2 to 10^3\n",
    "\n",
    "betas = [] # Store coefficients\n",
    "rmses = [] # Store validation rmses\n",
    "\n",
    "for a in alphas:\n",
    "    m = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        Ridge(alpha=a)\n",
    "    ).fit(X, y_train)\n",
    "    \n",
    "    betas.append((get_coefs(m)[1:]))\n",
    "    rmses.append(model_fit(m, X_test_matrix, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SbEdLZ5iS9m8"
   },
   "outputs": [],
   "source": [
    "res = pd.DataFrame(\n",
    "    data = betas,\n",
    "    columns = full_dummies.drop(['charges','smoker_no', 'sex_male'],axis=1).columns, # Label columns w/ feature names\n",
    ").assign(\n",
    "    alpha = alphas,\n",
    "    rmse = rmses\n",
    ").melt(\n",
    "    id_vars = ('alpha', 'rmse')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLjIIAFPUQOS"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='alpha', y='value', hue='variable', data=res).set_title(\"Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rigdk34wmSUB"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 12\n",
    "\n",
    "Based on this plot, which variable(s) seem to be the most important for predicting `charges`.\n",
    "\n",
    "Hint: think about what the degree of shrinkage towards 0 means in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3ci6mhPmf5j"
   },
   "source": [
    "---\n",
    "\n",
    "Add your text solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yXYoPEBtOeQ"
   },
   "source": [
    "---\n",
    "\n",
    "## 5.3. The Lasso\n",
    "\n",
    "The Lasso is a related modeling approach to Ridge regression, but instead uses an $\\ell_1$ penalty on the coefficients. Mathematically, we can express this model as the solution of the following optimization problem,\n",
    "\n",
    "$$ \\underset{\\boldsymbol{\\beta}}{\\text{argmin}} \\; \\lVert \\boldsymbol{y} - \\boldsymbol{X} \\boldsymbol{\\beta} \\rVert_2^2 + \\alpha \\lVert \\boldsymbol{\\beta} \\rVert_1. $$\n",
    "\n",
    "As with the other models from this worksheet, the `Lasso` model is also provided by the `linear_model` submodule and similarly requires the choice of the tuning parameter `alpha` to determine the weight of the $\\ell_1$ penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdqazB_HucNN"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "l = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    Lasso(alpha=10)\n",
    ").fit(X, y_train)\n",
    "model_fit(l, X_test_matrix, y_test, plot=True);\n",
    "print(\"lasso coefs:\", get_coefs(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyxOhimyu1bN"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 13\n",
    "\n",
    "Adjust the value of alpha in the cell above and rerun it. Qualitatively, how does the model fit change as alpha changes? How does the rmse change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpD5qAiUu-k_"
   },
   "source": [
    "---\n",
    "\n",
    "Add your text solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWmER8Owu-7j"
   },
   "source": [
    "### 5.3.1. Lasso $\\beta$s as a function of $\\alpha$\n",
    "\n",
    "As with Ridge regression we can examine the values of $\\beta$ we obtain as tuning parameter $\\alpha$ is adjusted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pCyax9HLvLpK"
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-2, 3, num=200)\n",
    "betas = [] # Store coefficients\n",
    "rmses = [] # Store validation rmses\n",
    "\n",
    "for a in alphas:\n",
    "    m = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        Lasso(alpha=a)\n",
    "    ).fit(X, y_train)\n",
    "\n",
    "    # Again ignore the intercept since it isn't included in the penalty\n",
    "    betas.append(get_coefs(m)[1:])  \n",
    "    rmses.append(model_fit(m, X_test_matrix, y_test))\n",
    "\n",
    "res = pd.DataFrame(\n",
    "    data = betas,       # Coefficients\n",
    "    columns = full_dummies.drop(['charges','smoker_no', 'sex_male'],axis=1).columns, # Coefficient names\n",
    ").assign(\n",
    "    alpha = alphas,     # Add alpahs\n",
    "    rmse = rmses        # Add validation rmses\n",
    ").melt(\n",
    "    id_vars = ('alpha', 'rmse') # Move columns into the rows\n",
    ")\n",
    "sns.lineplot(x='alpha', y='value', hue='variable', data=res).set_title(\"Coefficients\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b701lRkLzz5C"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 14\n",
    "\n",
    "Based on this plot, which variable(s) seem to be the most important for predicting `charges`.\n",
    "\n",
    "Hint: think about what the degree of shrinkage towards 0 means in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muyjy-HN0CpQ"
   },
   "source": [
    "---\n",
    "\n",
    "Add your text solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvLXpw4u0UUL"
   },
   "source": [
    "--- \n",
    "# 6. Model Refinement <a id='refine'></a>\n",
    "\n",
    "Linear regression doesn't have any hyperparameters to tune but ridge and lasso regression do ($\\alpha$). To close this workshop we will focus soley on model refinement for ridge regression, although the principles will also hold for lasso regreession."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUHDx0XZ5pV5"
   },
   "source": [
    "## 6.1 Tuning with GridSearchCV\n",
    "\n",
    "The $\\alpha$ in the Ridge regression model is another example of a hyperparameter and we can use cross validation to attempt to identify a good value for our data. We will start by using `GridSearchCV` to employ 5-fold cross validation to determine a good value $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oMYev3h6LXX"
   },
   "outputs": [],
   "source": [
    "\n",
    "alphas = np.linspace(0, 60, num=200)\n",
    "gs = GridSearchCV(\n",
    "    make_pipeline(\n",
    "        StandardScaler(),\n",
    "        Ridge()\n",
    "    ),\n",
    "    param_grid={'ridge__alpha': alphas},\n",
    "    cv=KFold(5, shuffle=True, random_state=1234),\n",
    "    scoring=\"neg_root_mean_squared_error\"\n",
    ").fit(X, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVnGW0NC6Yo6"
   },
   "source": [
    "Note that we are passing `sklearn.model_selection.KFold(5, shuffle=True, random_state=1234)` to the `cv` argument rather than leaving it to its default. This is because, sometimes the data is structured (sorted by some value) and this way we are able to ensure that the folds are properly shuffled. Failing to do this causes *very* unreliable results from the cross validation process.\n",
    "\n",
    "Once fit, we can examine the results to determine what value of $\\alpha$ was chosen as well as examine the calculated mean of the rmses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9RluUsl6hJZ"
   },
   "outputs": [],
   "source": [
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-psRNIO2pjVS"
   },
   "source": [
    "To evaluate this model we can access the best_estimator_ model object and use it to obtain an rmse for our validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fjdl_47QN2Oj"
   },
   "outputs": [],
   "source": [
    "model_fit(gs.best_estimator_, X_test_matrix, y_test, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snMF_kECp6d1"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 13\n",
    "\n",
    "How does this model compare to the performance of our baseline model? Is it better or worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A15mTdyBp_fB"
   },
   "source": [
    "---\n",
    "\n",
    "Add your text solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7Av7uHVqJJQ"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 14\n",
    "\n",
    "How do the model coefficient for this model compare to the base line model? Hint - be careful about which baseline model you compare with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvY3cVsdqPKa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Caoh8d9wqW7V"
   },
   "source": [
    "---\n",
    "\n",
    "Add your text solution here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flORzNRaqmfH"
   },
   "source": [
    "To further explore this choice of  Î±Î± , we can collect relevant data about the folds and their performance from the cv_results_ attribute. In this case we are particularly interested in examining the mean_test_score and the split#_test_score keys since these are used to determine the optimal  Î±Î± .\n",
    "\n",
    "In the code below we extract these data into a data frame by selecting our columns of interest along with the  Î±Î±  values used (and transform negative rmse values into positive values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VaHFPYmXPDRi"
   },
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame(\n",
    "    data = gs.cv_results_\n",
    ").filter(\n",
    "    # Extract the split#_test_score and mean_test_score columns\n",
    "    regex = '(split[0-9]+|mean)_test_score'\n",
    ").assign(\n",
    "    # Add the alphas as a column\n",
    "    alpha = alphas\n",
    ")\n",
    "\n",
    "cv_res.update(\n",
    "    # Convert negative rmses to positive\n",
    "    -1 * cv_res.filter(regex = '_test_score')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vlrEc2xtFSS"
   },
   "source": [
    "This data frame can then be used to plot $\\alpha$ against the mean root mean squared value over the 5 folds, to produce the following plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JLqO4rOPIWz"
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x='alpha', y='mean_test_score', data=cv_res)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEQWg8hWtw3N"
   },
   "outputs": [],
   "source": [
    "d = cv_res.melt(\n",
    "    id_vars=('alpha','mean_test_score'),\n",
    "    var_name='fold',\n",
    "    value_name='rmse'\n",
    ")\n",
    "\n",
    "sns.lineplot(x='alpha', y='rmse', color='black', data = d)  # Plot the mean rmse +/- the std dev of the rmse.\n",
    "sns.lineplot(x='alpha', y='rmse', hue='fold', data = d) # Plot the curves for each fold\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00086-7422c6b7-c86a-41ac-8d32-8ba6063dcba5",
    "deepnote_cell_type": "markdown",
    "id": "-ICOV5l8vZ8G"
   },
   "source": [
    "---\n",
    "\n",
    "## 7. Competing the worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the preceeding exercises. Now \n",
    "is a good time to check the reproducibility of this document by restarting the notebook's\n",
    "kernel and rerunning all cells in order.\n",
    "\n",
    "Once that is done and you are happy with everything, you can generate your PDF and turn it in on gradescope under the `mlp-week05` assignment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week05.ipynb",
   "provenance": []
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "2f0f4e4a-50b4-476a-ac32-ea3a1e98d30c",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
