{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "view-in-github",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Eldave93/mlp-handover/blob/main/workshops/week-06/6_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-oxide",
   "metadata": {
    "id": "accepting-oxide"
   },
   "source": [
    "# Week 06 - Non-Linear Regression\n",
    "by Colin Rundel & David Elliott & Kit Searle\n",
    "\n",
    "1. [Setup](#setup)\n",
    "\n",
    "2. [Baseline model](#base)\n",
    "\n",
    "3. [Polynomial regression](#poly)\n",
    "\n",
    "4. [Regression trees](#RT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-arcade",
   "metadata": {
    "id": "fantastic-arcade"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 1. Setup <a id='setup'></a>\n",
    "\n",
    "## 1.1. Uploading your data to the colab\n",
    "This notebook will be saved in your google drive in a folder \"Colab Notebooks\" by default, you should be fairly familiar with this by now.\n",
    "\n",
    "When you run this cell you will need to give colab permission to access files in your drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ABnI-jq0g1",
   "metadata": {
    "id": "28ABnI-jq0g1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "os.chdir('drive/My Drive/Colab Notebooks/mlp/week-6')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0NHGkk8cq7lE",
   "metadata": {
    "id": "0NHGkk8cq7lE"
   },
   "source": [
    "We will now unzip the workshop materials and place them in a subdirectory \"ws-material/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l3a7KO9eq9x_",
   "metadata": {
    "id": "l3a7KO9eq9x_"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('ws-material'):\n",
    "  print('Unzipping materials...')\n",
    "  !unzip week-06.zip -d ws-material\n",
    "else:\n",
    "  print(\"Directory already exists!\")\n",
    "\n",
    "os.chdir('ws-material')\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8uZLbLlUryNP",
   "metadata": {
    "id": "8uZLbLlUryNP"
   },
   "source": [
    "## 1.2. Packages\n",
    "\n",
    "Now lets load in the packages you wil need for this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-football",
   "metadata": {
    "collapsed": true,
    "id": "independent-football"
   },
   "outputs": [],
   "source": [
    "# Display plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting defaults\n",
    "plt.rcParams['figure.figsize'] = (8,5)\n",
    "plt.rcParams['figure.dpi'] = 80\n",
    "\n",
    "# sklearn modules\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-salon",
   "metadata": {
    "id": "metric-salon"
   },
   "source": [
    "## 1.3 Data\n",
    "\n",
    "The data set we will be using today is synthetic data that was generated via a random draw from a Gaussian Process model. The resulting data represent an unknown smooth function $y = f(x) + \\epsilon$. We will be implementing a variety of approaches for deriving a parameterized model of this function using least squares regression.\n",
    "\n",
    "We can read the data in from `gp.csv` and generate a scatter plot to get a sense of the shape of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-colombia",
   "metadata": {
    "id": "toxic-colombia"
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"./Data/gp.csv\")\n",
    "n = d.shape[0] # number of rows\n",
    "\n",
    "sns.scatterplot(x='x', y='y', data=d, color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I760MBDbMsmI",
   "metadata": {
    "id": "I760MBDbMsmI"
   },
   "source": [
    "---\n",
    "\n",
    "#2. Baseline model <a id='base'></a>\n",
    "\n",
    "Lets start by making a baseline model to compare to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sjTP6KaJAJZe",
   "metadata": {
    "id": "sjTP6KaJAJZe"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 1\n",
    "\n",
    "Fit a linear regression model to the training data, and create a plot of the regression line overlayed on the original data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JaXp9XxlBWNi",
   "metadata": {
    "id": "JaXp9XxlBWNi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "oeH9gKbrIl4V",
   "metadata": {
    "id": "oeH9gKbrIl4V"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 2\n",
    "\n",
    "Calculate the mean squared error of your linear model's predictions. The function `mean_squared_error` from `sklearn.metrics` will be useful for this. See the functions documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1LLJ66qsJItn",
   "metadata": {
    "id": "1LLJ66qsJItn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "modified-picture",
   "metadata": {
    "id": "modified-picture"
   },
   "source": [
    "---\n",
    "\n",
    "# 3. Polynomial regression <a id='poly'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "En9qNqrMNmra",
   "metadata": {
    "id": "En9qNqrMNmra"
   },
   "source": [
    "Polynomial regression is a straight forward approach to capturing simple non-linear relationships between a feature and our response variable. At its core, polynomial regression amounts to the inclusion of additional columns in the model matrix that are powers of the feature of interest.\n",
    "\n",
    "## 3.1 By hand\n",
    "\n",
    "For a single feature and low order polynomials this is can be done by hand. For example, if we want to fit a quadratic model to these data we can do the following,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-success",
   "metadata": {
    "collapsed": true,
    "id": "czech-success"
   },
   "outputs": [],
   "source": [
    "y = d.y\n",
    "X = np.c_[\n",
    "    np.ones(n),\n",
    "    d.x,\n",
    "    d.x**2\n",
    "]\n",
    "\n",
    "l = LinearRegression(fit_intercept = False).fit(X,y)\n",
    "\n",
    "print(l.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Eq8t7AeJOAPS",
   "metadata": {
    "id": "Eq8t7AeJOAPS"
   },
   "source": [
    "Which gives us the following model for the data,\n",
    "\n",
    "$$ \\hat{y} = 1.96 - 3.43 x + 1.30 x^2 $$\n",
    "\n",
    "The predictons of this model are then,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xfpFlQxcOiDW",
   "metadata": {
    "id": "xfpFlQxcOiDW"
   },
   "outputs": [],
   "source": [
    "d[\"pred_d2\"] = l.predict(X)\n",
    "\n",
    "sns.scatterplot(x='x', y='y', data=d, color=\"black\")\n",
    "sns.lineplot(x='x', y='value', hue=\"variable\", data=pd.melt(d,id_vars=[\"x\",\"y\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kTaBn5FJPAD2",
   "metadata": {
    "id": "kTaBn5FJPAD2"
   },
   "source": [
    "Note that the predicted values from this model form a curve rather than a straight line - specifically we have found the quadratic curve that best fits our data. We have used the pandas function melt to restructure our data frame, this is not strictly needed here but will be necessary for later examples. See the pandas documentation on melt for futher details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7sp6nYVtPINC",
   "metadata": {
    "id": "7sp6nYVtPINC"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 3\n",
    "\n",
    "Calculate the mean square error of this quadratic polynomial model. How does it compare to the linear model we previously fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UqRviVxyPM6Q",
   "metadata": {
    "id": "UqRviVxyPM6Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "g7uboLgkTFcF",
   "metadata": {
    "id": "g7uboLgkTFcF"
   },
   "source": [
    "Add your text solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WaEKfniOTLsO",
   "metadata": {
    "id": "WaEKfniOTLsO"
   },
   "source": [
    "---\n",
    "\n",
    "A cubic model can be similarly fit by adding a column containing $x^3$ to the previous model matrix (`X`). Note that by convention when fitting a polynomial model of degree $n$ we include all powers from $0$ to $n$, so for a cubic model our model matrix should include $x^0$, $x^1$, $x^2$, and $x^3$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "axCOgdo8TOSf",
   "metadata": {
    "id": "axCOgdo8TOSf"
   },
   "outputs": [],
   "source": [
    "X = np.c_[X, d.x**3]\n",
    "l = LinearRegression(fit_intercept = False).fit(X,y)\n",
    "\n",
    "print(l.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PNr7on63TY25",
   "metadata": {
    "id": "PNr7on63TY25"
   },
   "source": [
    "This gives us a cubic model with the form,\n",
    "\n",
    "$$\n",
    "\\hat{y} = 2.37 - 8.49 x + 13.9 x^2 - 8.39 x^3\n",
    "$$\n",
    "\n",
    "and predictons of this model are then,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F5NbP2f7TfUg",
   "metadata": {
    "id": "F5NbP2f7TfUg"
   },
   "outputs": [],
   "source": [
    "d[\"pred_d3\"] = l.predict(X)\n",
    "\n",
    "sns.scatterplot(x='x', y='y', data=d, color=\"black\")\n",
    "sns.lineplot(x='x', y='value', hue=\"variable\", data=pd.melt(d,id_vars=[\"x\",\"y\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-9bBMxbITwb3",
   "metadata": {
    "id": "-9bBMxbITwb3"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 4\n",
    "\n",
    "Calculate the mean square error of the cubic polynomial model. How does this model compare to your previous models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bLAOODtdT3mj",
   "metadata": {
    "id": "bLAOODtdT3mj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "kwshBaLPT7XW",
   "metadata": {
    "id": "kwshBaLPT7XW"
   },
   "source": [
    "Add your text solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BjMwMsAJ_W9b",
   "metadata": {
    "id": "BjMwMsAJ_W9b"
   },
   "source": [
    "---\n",
    "\n",
    "## 3.2 sklearn and polynomial features\n",
    "\n",
    "sklearn has a built in function called `PolynomialFeatures` which can be used to simplify the process of including polynomial features in a model. This function is included in the *preprocessing* module of sklearn, as with other python functions we can import it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Sy571J0z_acU",
   "metadata": {
    "id": "Sy571J0z_acU"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vc68ZaO3_dJo",
   "metadata": {
    "id": "Vc68ZaO3_dJo"
   },
   "source": [
    "Construction and use of this and other transformers is similar to what we have already seen with LinearRegression, we construct a PolynomialFeatures object in which we set basic options (e.g. the degree of the polynomial) and then apply the transformation to our data via fit_transform method. This will generate a new model matrix which includes the polynomial features up to the degree we have specified.\n",
    "\n",
    "To demonstrate the core features we will start with a toy example and then replicate the models we constructed in Section 3.1. Below we construct our sample data vector and then pass it into the transformer,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_6e4dOHNEr88",
   "metadata": {
    "id": "_6e4dOHNEr88"
   },
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4]) # Initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vXZar3CyEtkg",
   "metadata": {
    "id": "vXZar3CyEtkg"
   },
   "outputs": [],
   "source": [
    "PolynomialFeatures(degree = 2).fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m-STKXlKE8HO",
   "metadata": {
    "id": "m-STKXlKE8HO"
   },
   "source": [
    "When we run the above code we get an error because currently x is a 1D vector when the fit_transform function is expected a 2D array (the same will also happen with LinearRegression's fit method). To solve this we need to make sure that the value we pass has the correct dimensions, x.reshape(-1, 1) is suggested by the error and corrects the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hqVlLBqfE99s",
   "metadata": {
    "id": "hqVlLBqfE99s"
   },
   "outputs": [],
   "source": [
    "PolynomialFeatures(degree = 2).fit_transform(x.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amH7115BFCS0",
   "metadata": {
    "id": "amH7115BFCS0"
   },
   "source": [
    "Alternatively, we can alsu use the np.c_ function to construct the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rPAqQ8dcFFVP",
   "metadata": {
    "id": "rPAqQ8dcFFVP"
   },
   "outputs": [],
   "source": [
    "PolynomialFeatures(degree = 2).fit_transform(np.c_[x]) # Order 2 polynomial model matrix for x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CXn4Cq-lFM4x",
   "metadata": {
    "id": "CXn4Cq-lFM4x"
   },
   "outputs": [],
   "source": [
    "PolynomialFeatures(degree = 3).fit_transform(np.c_[x]) # Order 3 polynomial model matrix for x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hiEIPX59FTm8",
   "metadata": {
    "id": "hiEIPX59FTm8"
   },
   "source": [
    "Note that when we use this transform we get all of the polynomial transformations of x from 0 to degree. In this case, the 0 degree column is equivalent to the intercept column. If for some reason we did not want to include this we can construct PolynomialFeatures with include_bias=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vQ4Bd57jFX5a",
   "metadata": {
    "id": "vQ4Bd57jFX5a"
   },
   "outputs": [],
   "source": [
    "PolynomialFeatures(degree = 3, include_bias=False).fit_transform(np.c_[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51AbDcWLFeke",
   "metadata": {
    "id": "51AbDcWLFeke"
   },
   "source": [
    "We can now apply this to our original data by storing the result as X then passing this new model matrix to the linear regression fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ivjt9oHhFiDg",
   "metadata": {
    "id": "Ivjt9oHhFiDg"
   },
   "outputs": [],
   "source": [
    "X = PolynomialFeatures(degree = 3).fit_transform(np.c_[d.x])\n",
    "\n",
    "l = LinearRegression(fit_intercept = False).fit(X,d.y)\n",
    "print(l.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SYMdnZFpFsTo",
   "metadata": {
    "id": "SYMdnZFpFsTo"
   },
   "source": [
    "These values match those we obtained from the earlier cubic model, and we can also see this when plotting the model predictons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Go9i4yguFvG1",
   "metadata": {
    "id": "Go9i4yguFvG1"
   },
   "outputs": [],
   "source": [
    "d[\"pred_d3_sk\"] = l.predict(X)\n",
    "\n",
    "sns.scatterplot(x='x', y='y', data=d, color=\"black\")\n",
    "sns.lineplot(x='x', y='value', hue=\"variable\", data=pd.melt(d,id_vars=[\"x\",\"y\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5Ehhqy6mF5My",
   "metadata": {
    "id": "5Ehhqy6mF5My"
   },
   "source": [
    "We can not even see the pred_d3 line as it is being overplotted by the pred_d3_sk line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MlM-Pk5hSZzk",
   "metadata": {
    "id": "MlM-Pk5hSZzk"
   },
   "source": [
    "## 3.3 Pipelines\n",
    "\n",
    "You may have noticed that `PolynomialFeatures` takes a model matrix as input and returns a new model matrix as output which is then used as the input for `LinearRegression`. This is not an accident, and by structuring the library in this way sklearn is designed to enable the connection of these steps together, into what sklearn calls a *pipeline*.\n",
    "\n",
    "We can modularize and simply our code somewhat by creating a pipeline that takes our original data, performs a polynomial feature transform and then feeds the results into a linear regression. We can accomplish this via the `make_pipeline` function from the `pipeline` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vDjYa1k4Shww",
   "metadata": {
    "id": "vDjYa1k4Shww"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "poly_model = make_pipeline(\n",
    "    PolynomialFeatures(degree=4),\n",
    "    LinearRegression()\n",
    ")\n",
    "\n",
    "print(poly_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y7H1TDrRSm0y",
   "metadata": {
    "id": "y7H1TDrRSm0y"
   },
   "source": [
    "The resulting object represents a new \"model\" which can then be fit to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FyAWJNSUSpKW",
   "metadata": {
    "id": "FyAWJNSUSpKW"
   },
   "outputs": [],
   "source": [
    "p = poly_model.fit(np.c_[d.x], d.y)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-bzACuq_Ss8x",
   "metadata": {
    "id": "-bzACuq_Ss8x"
   },
   "source": [
    "and used to create predictions just like our previous `LinearRegression` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4N6TTWOfSvXg",
   "metadata": {
    "id": "4N6TTWOfSvXg"
   },
   "outputs": [],
   "source": [
    "d['pred_d4_pipe'] = p.predict(np.c_[d.x])\n",
    "\n",
    "sns.scatterplot(x='x', y='y', data=d, color=\"black\")\n",
    "sns.lineplot(x='x', y='value', hue=\"variable\", data=pd.melt(d,id_vars=[\"x\",\"y\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IrSDIOI1S4Rb",
   "metadata": {
    "id": "IrSDIOI1S4Rb"
   },
   "source": [
    "The returned object is a `Pipeline` object so it will not provide direct access to step properties, such as the coefficients for the regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jwKvHqRrS60i",
   "metadata": {
    "id": "jwKvHqRrS60i"
   },
   "outputs": [],
   "source": [
    "print(p.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V9loUbEPS-tu",
   "metadata": {
    "id": "V9loUbEPS-tu"
   },
   "source": [
    "If we want access to the attributes or methods of a particular step we need to first access that step using either its name or position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iN2LpbkqTBfx",
   "metadata": {
    "id": "iN2LpbkqTBfx"
   },
   "outputs": [],
   "source": [
    "print(p.named_steps['linearregression'].coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kYXTpXTVTFjD",
   "metadata": {
    "id": "kYXTpXTVTFjD"
   },
   "outputs": [],
   "source": [
    "print(p.steps[1][1].intercept_) # second subset is necessary here because \n",
    "                                # each step is a tuple of a name and the \n",
    "                                # model / transform object "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XRG7GFy-TPlp",
   "metadata": {
    "id": "XRG7GFy-TPlp"
   },
   "source": [
    "Also note that both the `'linearregression'` and `'polynomialfeatures'` step introduce a column of ones into the model matrix. This potentially introduces a rank deficiency in our model matrix, however sklearn handles this by fixing the coefficient of the duplicate intercept column to 0. The result however is that the intercept is now stored in `intercept_` and not `coef_`. If we wished to avoid this we would need to construct our pipeline using `LinearRegression(fit_intercept=False)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kVXrjqAMTSm0",
   "metadata": {
    "id": "kVXrjqAMTSm0"
   },
   "outputs": [],
   "source": [
    "p = make_pipeline(\n",
    "    PolynomialFeatures(degree=4),\n",
    "    LinearRegression(fit_intercept = False)\n",
    ").fit(\n",
    "    np.c_[d.x],\n",
    "    d.y\n",
    ")\n",
    "\n",
    "print(p.named_steps['linearregression'].coef_)\n",
    "print(p.named_steps['linearregression'].intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6gaptWgCTbGP",
   "metadata": {
    "id": "6gaptWgCTbGP"
   },
   "source": [
    "## 3.4 Putting it all together\n",
    "\n",
    "So far each time we've wanted to fit a polynomial regression model we've had to set up the various data objects and then create the sklearn model and transformation objects and then get a final result of the coefficients or a plot - because of this the preceeding cells contain a lot of duplicated code which is something we should be trying to avoid. \n",
    "\n",
    "Our modeling task here is like many other programming tasks, when we find ourselves reusing the same code over and over it is time to consider writing a function. Below I have implemented a function called `poly_reg` that allows the user to provide a data frame and specific the names of the `x` column an d `y` column as well as the degree of the polynomial we would like to fit. The function returns the original data frame with the model predictions added as well as the model coefficients in a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hKaZkpOjTgCB",
   "metadata": {
    "id": "hKaZkpOjTgCB"
   },
   "outputs": [],
   "source": [
    "def poly_reg(data, x = 'x', y = 'y', degree=1, plot_data = True, plot_fit = True):\n",
    "    X = np.c_[data[x]]\n",
    "    Y = data[y]\n",
    "    \n",
    "    p = make_pipeline(\n",
    "        PolynomialFeatures(degree=degree),\n",
    "        LinearRegression(fit_intercept=False)\n",
    "    )\n",
    "    \n",
    "    m = p.fit(X,Y)\n",
    "    \n",
    "    pred_col = 'pred_d'+str(degree)\n",
    "    data[pred_col] = m.predict(X)\n",
    "\n",
    "    if plot_data:\n",
    "        sns.scatterplot(x=data[x], y=data[y], color=\"black\")\n",
    "\n",
    "    if plot_fit:\n",
    "        sns.lineplot(x=data[x], y=data[pred_col])\n",
    "    \n",
    "    return (data, m.steps[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HUsAPss8Tjh9",
   "metadata": {
    "id": "HUsAPss8Tjh9"
   },
   "outputs": [],
   "source": [
    "_, _ = poly_reg(d, degree=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nlSVqOkKTwTd",
   "metadata": {
    "id": "nlSVqOkKTwTd"
   },
   "source": [
    "Since we are not using either the data frame or model coefficients we unpack them into `_`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kyS6kBbMzssV",
   "metadata": {
    "id": "kyS6kBbMzssV"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 5\n",
    "\n",
    "Use the `poly_reg` function to fit a variety of different polynomial models to the data. What values of `degree` provides the best fit (judged qualitatively not quantitatively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2zdwRNVPz-5R",
   "metadata": {
    "id": "2zdwRNVPz-5R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tMWIYDiu0B8t",
   "metadata": {
    "id": "tMWIYDiu0B8t"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MF41FPek0BXx",
   "metadata": {
    "id": "MF41FPek0BXx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0WrN79Az0Ayl",
   "metadata": {
    "id": "0WrN79Az0Ayl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "LoXXdUXK04Dd",
   "metadata": {
    "id": "LoXXdUXK04Dd"
   },
   "source": [
    "Add your text solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qk2sjtWE1C-8",
   "metadata": {
    "id": "qk2sjtWE1C-8"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 6\n",
    "\n",
    "\n",
    "In the cell below we have provided a copy of the `poly_reg` function, modify this function such that it returns a 3rd value that is the mean square error of the predictions for the model. Check that the value returned by the function matches the MSE you calculated in **Exercises 2-4**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HBCZFkjf1bje",
   "metadata": {
    "id": "HBCZFkjf1bje"
   },
   "outputs": [],
   "source": [
    "def poly_reg_mse(data, x = 'x', y = 'y', degree=1, plot_data = True, plot_fit = True):\n",
    "    X = np.c_[data[x]]\n",
    "    Y = data[y]\n",
    "    \n",
    "    p = make_pipeline(\n",
    "        PolynomialFeatures(degree=degree),\n",
    "        LinearRegression(fit_intercept=False)\n",
    "    )\n",
    "    \n",
    "    m = p.fit(X,Y)\n",
    "    \n",
    "    pred_col = 'pred_d'+str(degree)\n",
    "    data[pred_col] = m.predict(X)\n",
    "\n",
    "    if plot_data:\n",
    "        sns.scatterplot(data[x],data[y], color=\"black\")\n",
    "\n",
    "    if plot_fit:\n",
    "        sns.lineplot(data[x],data[pred_col])\n",
    "    \n",
    "    # Only the line below needs to change\n",
    "    return (data, m.steps[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NA--oVPL1n7e",
   "metadata": {
    "id": "NA--oVPL1n7e"
   },
   "source": [
    "Below are three calls to this function which report these MSE values.\n",
    "\n",
    "Hint: Note that this updated function should return three objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FEilTSFM1ljp",
   "metadata": {
    "id": "FEilTSFM1ljp"
   },
   "outputs": [],
   "source": [
    "_, _, mse_d1 = poly_reg_mse(d, degree=1, plot_data=False, plot_fit=False)\n",
    "_, _, mse_d2 = poly_reg_mse(d, degree=2, plot_data=False, plot_fit=False)\n",
    "_, _, mse_d3 = poly_reg_mse(d, degree=3, plot_data=False, plot_fit=False)\n",
    "\n",
    "\n",
    "print(\"deg=1:\", mse_d1)\n",
    "print(\"deg=2:\", mse_d2)\n",
    "print(\"deg=3:\", mse_d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enP7t4Vb1ros",
   "metadata": {
    "id": "enP7t4Vb1ros"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 7\n",
    "\n",
    "\n",
    "Construct a data frame called `poly_res` with the columns `degree` and `mse`. `degree` should contain the integer values 1 to 20 and `mse` should contain the mean squared errors, calculated using the function `poly_reg_mse`, for each of the different `degree` values.\n",
    "\n",
    "Hint: have a look at the folowing pandas function https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.assign.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kl6Tswnw1xbj",
   "metadata": {
    "id": "kl6Tswnw1xbj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dFQi2JWv18l5",
   "metadata": {
    "id": "dFQi2JWv18l5"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 8\n",
    "\n",
    "Create a plot of `degree` vs. `mse` using the `poly_res` data frame you created above. Based on this comment on what model appears to best fit the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hoafUpTI2FnU",
   "metadata": {
    "id": "hoafUpTI2FnU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aY5J_5W8w8YN",
   "metadata": {
    "id": "aY5J_5W8w8YN"
   },
   "source": [
    "Add your text solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G4aznf4x2QLO",
   "metadata": {
    "id": "G4aznf4x2QLO"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 9\n",
    "\n",
    "Based on your findings in the preceding exercises, can we find the \"optimal model\" using this kind of approach? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VEpPtpdW2XCh",
   "metadata": {
    "id": "VEpPtpdW2XCh"
   },
   "source": [
    "Add your text solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZeDdMEzlQ9vW",
   "metadata": {
    "id": "ZeDdMEzlQ9vW"
   },
   "source": [
    "## 3.5 Chosing the order of the polynomial\n",
    "\n",
    "We saw above that increasing the order of the polynomial apears to provide a better fit to the data. In the lecture we discussed how chosing $M$ to be too large can cause over fittting. When we over fit a polynomial regression model, the MSE for the training data will apear to be low which might indicate that the model is a good fit. As a result of over fitting, the MSE for the predictions of the unseen test data may begin to decrease. We will illustrate this next.\n",
    "\n",
    "First, we will read in clean data and create our training and testing subsets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZJ-3PZ7Zafoe",
   "metadata": {
    "id": "ZJ-3PZ7Zafoe"
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"./Data/gp.csv\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = d.drop('y',axis=1) # Independet variable\n",
    "y = d['y'] # dependent variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=23)\n",
    "\n",
    "#full datasets for plotting purposes\n",
    "train_full = X_train.join(y_train)\n",
    "test_full = X_test.join(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zIf5SNRXh1re",
   "metadata": {
    "id": "zIf5SNRXh1re"
   },
   "source": [
    "Next, we will update our `poly_reg_mse` function so that it can take as input a training dataset and a testing data set. The new function will return the mse for both the train and test dataset (as well as a plot with black representing the training points and red representing the testing points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N1QEL3vQQ9Yo",
   "metadata": {
    "id": "N1QEL3vQQ9Yo"
   },
   "outputs": [],
   "source": [
    "def poly_reg_mse_2(train_data,test_data, x = 'x', y = 'y', degree=1, plot_data = True, plot_fit = True):\n",
    "    X = np.c_[train_data[x]]\n",
    "    Y = train_data[y]\n",
    "    \n",
    "    p = make_pipeline(\n",
    "        PolynomialFeatures(degree=degree),\n",
    "        LinearRegression(fit_intercept=False)\n",
    "    )\n",
    "    \n",
    "    m = p.fit(X,Y)\n",
    "    \n",
    "    pred_col = 'pred_d'+str(degree)\n",
    "    train_data[pred_col] = m.predict(X)\n",
    "    test_data[pred_col] = m.predict(np.c_[test_data[x]])\n",
    "    if plot_data:\n",
    "        sns.scatterplot(train_data[x],train_data[y], color=\"black\")\n",
    "        sns.scatterplot(test_data[x],test_data[y], color=\"red\")\n",
    "\n",
    "    if plot_fit:\n",
    "        sns.lineplot(train_data[x],train_data[pred_col])\n",
    "\n",
    "    test_mse = mean_squared_error(test_data[y], test_data[pred_col])\n",
    "    train_mse = mean_squared_error(train_data[y], train_data[pred_col])\n",
    "  \n",
    "    return (test_data, m.steps[1][1], train_mse, test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N4_UqMjWVsUx",
   "metadata": {
    "id": "N4_UqMjWVsUx"
   },
   "outputs": [],
   "source": [
    "_, _, mse_test, mse_train = poly_reg_mse_2(train_full,test_full, degree=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OxYJ8bboif8I",
   "metadata": {
    "id": "OxYJ8bboif8I"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 10\n",
    "\n",
    "Change the values for degree. What do you notice about the fit as we increase the polynomial degree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rLiiGsuki0X7",
   "metadata": {
    "id": "rLiiGsuki0X7"
   },
   "source": [
    "Add your text solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X0U9mK_Fv2D9",
   "metadata": {
    "id": "X0U9mK_Fv2D9"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 11\n",
    "\n",
    "\n",
    "Construct a data frame called `poly_res_2` with the columns `degree`, `mse_train` and `mse_test`. `degree` should contain the integer values 1 to 80 and `mse_train` and `mse_test` should contain the mean squared errors computed using the training data and testing data, respectively, calculated using the function `poly_reg_mse_2`, for each of the different `degree` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kK_blX7cS5Ro",
   "metadata": {
    "id": "kK_blX7cS5Ro"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "N5Ahs_ogxOTf",
   "metadata": {
    "id": "N5Ahs_ogxOTf"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 12\n",
    "\n",
    "Create a plot of `degree` vs. `mse_train` and `mse_test` using the `poly_res_2` data frame you created above. Based on this comment on what model appears to best fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gQ88lvMZz3eb",
   "metadata": {
    "id": "gQ88lvMZz3eb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "IWYuXg6a13xE",
   "metadata": {
    "id": "IWYuXg6a13xE"
   },
   "source": [
    "Add your text solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pigDbUf16pW4",
   "metadata": {
    "id": "pigDbUf16pW4"
   },
   "source": [
    "If we wish to test over a specific set of parameter values using cross validation we can use the `GridSearchCV` function from the `model_selection` submodule.\n",
    "\n",
    "This argument is a dictionary containing parameters names as keys and lists of parameter settings to try as values. Since we are using a pipeline, out parameter name will be the name of the pipeline step, `polynomialfeatures`, followed by `__`, and then the parameter name, `degree`. So for our pipeline the parameter is named `polynomialfeatures__degree`. If you want to list any models available parameters you can call the `get_params()` method on the model object, e.g. `m.get_params()` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kPAFmer36sHX",
   "metadata": {
    "id": "kPAFmer36sHX"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "m = make_pipeline(\n",
    "        PolynomialFeatures(),\n",
    "        LinearRegression(fit_intercept=False)\n",
    "    )\n",
    "\n",
    "parameters = {\n",
    "    'polynomialfeatures__degree': np.arange(1,31,1)\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "\n",
    "grid_search = GridSearchCV(m, parameters, cv=kf, scoring=\"neg_mean_squared_error\").fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZilDMAMc6vk5",
   "metadata": {
    "id": "ZilDMAMc6vk5"
   },
   "source": [
    "The above code goes through the process of fitting all $5 \\times 30$ models as well as storing and ranking the results for the requested scoring metric(s). Note that here we have used \"neg_mean_squared_error\" as our scoring metric which returns the negative of the root mean squared error. As the name implies this returns the negative of the usual fit metric, this is because sklearn expects to always optimize for the maximum of a score and the model with the largest negative MSE will therefore be the \"best\". \n",
    "\n",
    "In this workshop we have used MSE as a metric for testing our models. This metric is entirely equivalent to the root mean squared error for purposes of ranking / ordering models (as the square root is a monotonic transformation). Sometimes the rmse is prefered as it is more interpretable, because it has the same units as $y$.\n",
    "\n",
    "Once all of the submodels are fit, we can determine the optimal hyperparameter value by accessing the object's `best_*` attributes,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C6P48m5T6ym9",
   "metadata": {
    "id": "C6P48m5T6ym9"
   },
   "outputs": [],
   "source": [
    "print(\"best index: \", grid_search.best_index_)\n",
    "print(\"best param: \", grid_search.best_params_)\n",
    "print(\"best score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-kelly",
   "metadata": {
    "id": "adopted-kelly"
   },
   "source": [
    "# 4. Regression trees (via feature engineering) <a id='RT'></a>\n",
    "\n",
    "\n",
    "Reacll that regression trees assume the basis function of the form,\n",
    "\n",
    "$$\n",
    "f_j(x; a_j,b_j) = \\begin{cases}\n",
    "1 & \\text{if $a_j \\leq x < b_j$} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This reflects one possible basis function that we might choose to better fit our data. In order to use these functions to transform our data, we will need to choose the values of $a$ and $b$ as well as the number of basis functions to use $M$. Given those choices our goal is to then to fit the model,\n",
    "\n",
    "$$\n",
    "y_i = \\beta_1 f(x_i; a_1,b_1) + \\beta_2 f(x_i; a_2,b_2) + \\ldots +  \\beta_M f(x_i; a_M,b_M) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dg7-wlD_3109",
   "metadata": {
    "id": "dg7-wlD_3109"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 13\n",
    "\n",
    "Explain why we are not able to directly use the least squares approach to find optimal values of $a$, $b$, and $M$ to fit our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_VeIicRF3_Ms",
   "metadata": {
    "id": "_VeIicRF3_Ms"
   },
   "source": [
    "Add your text solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l8ReSy9g4dQn",
   "metadata": {
    "id": "l8ReSy9g4dQn"
   },
   "source": [
    "---\n",
    "\n",
    "## 4.1 With pandas\n",
    "\n",
    "For our example today, we will keep things simple and divide our `x` values up into 11 equal sized pieces: $[0,0.1)$, $[0.1,0.2)$, $\\ldots$, $[0.9,1)$. This can be achieved using pandas' `cut` function as follows, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4Pq-cv54lVK",
   "metadata": {
    "id": "c4Pq-cv54lVK"
   },
   "outputs": [],
   "source": [
    "d = pd.read_csv(\"./Data/gp.csv\") # Get a clean copy of the data\n",
    "\n",
    "d[\"x_bin\"] = pd.cut(d.x, np.linspace(0,1,11))\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iV4NPGvc41un",
   "metadata": {
    "id": "iV4NPGvc41un"
   },
   "source": [
    "`np.linspace` is used here to specify the cut points and returns the array `[0,0.1,...,0.9,1]`. The results new column `x_bin` now contains a categorical variable for the interval containing the `x` value for that row. \n",
    "\n",
    "We can then use pandas' `get_dummies` to transform this categorical variable using one hot encoding. After brief examination you should be able to see that this is equivalent applying the 11 basis functions (columns) to each of the `x` values (rows) using the ranges we've described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EzYcyCux41Xk",
   "metadata": {
    "id": "EzYcyCux41Xk"
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(d.x_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vuh_nTGq4_Sl",
   "metadata": {
    "id": "Vuh_nTGq4_Sl"
   },
   "source": [
    "We can then fit a linear regression model using only these dummy variable and obtain the following model fit,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MPRD0icI49I8",
   "metadata": {
    "id": "MPRD0icI49I8"
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(d.x_bin)\n",
    "y = d.y\n",
    "l = LinearRegression(fit_intercept=False).fit(X.values,y)\n",
    "d[\"pred_M11\"] = l.predict(X.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7rxeEay8eAb",
   "metadata": {
    "id": "d7rxeEay8eAb"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x='x', y='y', data=d, color=\"black\")\n",
    "sns.lineplot(x='x', y='pred_M11', data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Da-ceyBhD5_R",
   "metadata": {
    "id": "Da-ceyBhD5_R"
   },
   "outputs": [],
   "source": [
    "l.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GvMxqNCL8jbD",
   "metadata": {
    "id": "GvMxqNCL8jbD"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 14\n",
    "\n",
    "Calculate the mean squared error of this model. How does it compare to the polynomial models we fit previously?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vKgFd2b78q4Y",
   "metadata": {
    "id": "vKgFd2b78q4Y"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "IwvA9bBx8whh",
   "metadata": {
    "id": "IwvA9bBx8whh"
   },
   "source": [
    "This model performs reasonably well, it is competative with polynomial models with degree <5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jiWeh0ER83Hz",
   "metadata": {
    "id": "jiWeh0ER83Hz"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 15\n",
    "\n",
    "Repeat this fitting procedure, but vary the number of bins being used (by changing the 3rd parameter in `np.linspace`). How does the MSE compare to the previous models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbFQkxQb8-E2",
   "metadata": {
    "id": "cbFQkxQb8-E2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2oyrxgzU9Lrn",
   "metadata": {
    "id": "2oyrxgzU9Lrn"
   },
   "source": [
    "Add your text solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eyyWLhvzDVih",
   "metadata": {
    "id": "eyyWLhvzDVih"
   },
   "source": [
    "---\n",
    "\n",
    "## 4.2 With sklearn\n",
    "\n",
    "A similar process can be achieved with sklearn's `KBinsDiscretizer` from the *preprocessing* submodule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pcBObk-SDYyp",
   "metadata": {
    "id": "pcBObk-SDYyp"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8NA4PNKDbMC",
   "metadata": {
    "id": "e8NA4PNKDbMC"
   },
   "source": [
    "The key arguments for this transformer are `n_bins` which determines the number of bins, `encode` which determines how the transformed values are encoded, and `strategy` which is used to determine bin widths. To (almost) replicate our results from pandas we will use `strategy=\"uniform\"` and `encode=\"onehot-dense\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ZzJ25mEDdjj",
   "metadata": {
    "id": "5ZzJ25mEDdjj"
   },
   "outputs": [],
   "source": [
    "kb_disc = KBinsDiscretizer(n_bins=10, strategy=\"uniform\", encode=\"onehot-dense\")\n",
    "kb_disc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qdo9jEpWDjXa",
   "metadata": {
    "id": "Qdo9jEpWDjXa"
   },
   "source": [
    "We can examine where the transformer has selected the bin edges to be located by calling the fit method on our features and then printing the `bin_edges_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KvMN90fIDllD",
   "metadata": {
    "id": "KvMN90fIDllD"
   },
   "outputs": [],
   "source": [
    "X = np.c_[d.x]\n",
    "kb_disc.fit(X).bin_edges_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GE4yEb7nDn9A",
   "metadata": {
    "id": "GE4yEb7nDn9A"
   },
   "source": [
    "These cut points differ slightly from those we obtained from pandas, this is happening because we are using the data to derive the cut points and since the x values are randomly generated the data does not begin exactly at 0 and end at 1.\n",
    "\n",
    "If we want to match pandas exactly, we can provide new data which is used to fit the transformer and then apply that to our data using the transform later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gTvIf4bWDp9u",
   "metadata": {
    "id": "gTvIf4bWDp9u"
   },
   "outputs": [],
   "source": [
    "x_range = np.array([0,1])\n",
    "kb_disc.fit(x_range.reshape(-1,1)).bin_edges_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6501mA-mEP_O",
   "metadata": {
    "id": "6501mA-mEP_O"
   },
   "source": [
    "To obtain the transformed feature, we need to call transform with the feature to transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LL6jjGQUESQ1",
   "metadata": {
    "id": "LL6jjGQUESQ1"
   },
   "outputs": [],
   "source": [
    "kb_disc.fit(x_range.reshape(-1,1)).transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZkN9_g-3EWLq",
   "metadata": {
    "id": "ZkN9_g-3EWLq"
   },
   "source": [
    "If instead we used fit_transform we will get the bin edges from the first example above, this will be the default behavior of the transformer if we use it in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_5kftNeVEX6q",
   "metadata": {
    "id": "_5kftNeVEX6q"
   },
   "outputs": [],
   "source": [
    "kb_disc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_Rm10gvdEcnT",
   "metadata": {
    "id": "_Rm10gvdEcnT"
   },
   "source": [
    "The two data frames above look similar, but are very slightly different due to the different bin edges used.\n",
    "\n",
    "Just like our polynomial regression model, we can combine this transformed with the model using a pipeline. This combined with some plotting and diagnostic code is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13PjihLJEevX",
   "metadata": {
    "id": "13PjihLJEevX"
   },
   "outputs": [],
   "source": [
    "def reg_tree(data, x = 'x', y = 'y', \n",
    "             n_bins=10, strategy = \"uniform\",\n",
    "             x_range = np.array([0,1]),\n",
    "             plot_data = True, plot_fit = True):\n",
    "    \n",
    "    X = np.c_[data[x]]\n",
    "    Y = data[y]\n",
    "    \n",
    "    p = make_pipeline(\n",
    "        KBinsDiscretizer(n_bins=n_bins, strategy=strategy, encode=\"onehot-dense\"),\n",
    "        LinearRegression(fit_intercept=False) # Since we are using onehot above we \n",
    "    )                                         # need to remove the intercept here\n",
    "    \n",
    "    m = p.fit(X,Y)\n",
    "    \n",
    "    pred_col = 'pred_rt_M'+str(n_bins)\n",
    "    data[pred_col] = m.predict(X)\n",
    "\n",
    "    if plot_data:\n",
    "        sns.scatterplot(x=data[x], y=data[y], color=\"black\")\n",
    "\n",
    "    if plot_fit:\n",
    "        sns.lineplot(x=data[x], y=data[pred_col])\n",
    "    \n",
    "    return (data, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6i8VfjECEgxv",
   "metadata": {
    "id": "6i8VfjECEgxv"
   },
   "outputs": [],
   "source": [
    "_, _  = reg_tree(d, n_bins=11, strategy=\"quantile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MBBA4t-xEoav",
   "metadata": {
    "id": "MBBA4t-xEoav"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 16\n",
    "\n",
    "Use the `reg_tree` function to fit a variety of different discretized models to the data. What value of `n_bins` provides the best fit (judged qualitatively not quantitatively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Oc7aOkPoEy9J",
   "metadata": {
    "id": "Oc7aOkPoEy9J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8WWXxw4EE6j9",
   "metadata": {
    "id": "8WWXxw4EE6j9"
   },
   "source": [
    "Add your text solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "X0tjIKNhFHqd",
   "metadata": {
    "id": "X0tjIKNhFHqd"
   },
   "source": [
    "---\n",
    "\n",
    "### ðŸš© Exercise 17\n",
    "\n",
    "Based on what you've seen in Exercise 13, how do you think the quality of fit changes as `n_bins` increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NEjtBkAzFMTK",
   "metadata": {
    "id": "NEjtBkAzFMTK"
   },
   "source": [
    "Add your text solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Cj43Ch0mpkDD",
   "metadata": {
    "id": "Cj43Ch0mpkDD"
   },
   "source": [
    "---\n",
    "\n",
    "## 5. Competing the worksheet\n",
    "\n",
    "At this point you have hopefully been able to complete all the preceeding exercises. Now  is a good time to check the reproducibility of this document by restarting the notebook's kernel and rerunning all cells in order.\n",
    "\n",
    "Once that is done and you are happy with everything, you can generate your PDF and turn it in on gradescope under the `mlp-week06` assignment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week06.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
