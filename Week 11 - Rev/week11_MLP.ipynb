{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11 - Review and Beyond MLP\n",
    "\n",
    "### Aims\n",
    "\n",
    "By the end of this notebook you will be able to understand \n",
    "\n",
    ">* The Basics of Keras overview\n",
    ">* Linear Regression by Keras\n",
    ">* Working out on Project II data\n",
    "\n",
    "The exercises here are designed to reinforce the basics of keras for further use. Additionally, you will see some simple tasks to try and comment out. \n",
    "\n",
    "- For the last time, you will have lighter tasks tagged by (CORE) and (EXTRA).\n",
    "\n",
    "- If you already submitted at least 5 hands-in script before (marked as 1), you can directly start your project II during the WS. \n",
    "\n",
    "- Some experiments asked below is related to the hotel data already. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm6XqPScK-FV"
   },
   "source": [
    "# Imports\n",
    "\n",
    "We're only going to need a couple of standard libraries this week, as well as keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display plots inline\n",
    "%matplotlib inline  \n",
    "\n",
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "usEZAe6SYpV8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not necessary in general !\n",
    "import os, datetime\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --port=5036 --logdir $logdir\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(logdir, histogram_freq=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeHcm_OBGhtx"
   },
   "source": [
    "# Basics\n",
    "\n",
    "**Just to highlight some differences**\n",
    "\n",
    "TensorFlow is an infrastructure layer for differentiable programming. At its heart, it's a framework for manipulating N-dimensional arrays (tensors), much like NumPy.But as you experienced already, there are three key differences between NumPy and TensorFlow:\n",
    "\n",
    "- TensorFlow can leverage hardware accelerators such as GPUs and TPUs.\n",
    "\n",
    "- TensorFlow can automatically compute the gradient of arbitrary differentiable tensor expressions.\n",
    "\n",
    "- TensorFlow computation can be distributed to large numbers of devices on a single machine, and large number of machines (potentially with multiple devices each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZw0Lb1wR395"
   },
   "outputs": [],
   "source": [
    "x = tf.constant([[5, 2], [1, 3]])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 2],\n",
       "       [1, 3]], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can get its value as a NumPy array by calling .numpy():\n",
    "x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype: <dtype: 'int32'>\n",
      "shape: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "# Much like a NumPy array, it features the attributes dtype and shape:\n",
    "print(\"dtype:\", x.dtype)\n",
    "print(\"shape:\", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 0.7950265 , -0.5505485 ],\n",
       "       [ 0.75827163,  0.11674298]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can also create random constant tensors:\n",
    "x = tf.random.normal(shape=(2, 2), mean=0.0, stddev=1.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Variables are special tensors used to store mutable state (such as the weights of a neural network). You create a Variable using some initial value:\n",
    "\n",
    "**Doing math in TensorFlow:** \n",
    "\n",
    "If you've used NumPy, doing math in TensorFlow will look very familiar. The main difference is that your TensorFlow code can run on GPU and TPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[-1.0459622 ,  0.7224666 ],\n",
      "       [-0.13016361, -0.787909  ]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "initial_value = tf.random.normal(shape=(2, 2))\n",
    "a = tf.Variable(initial_value)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.4076662   0.01619387]\n",
      " [-0.38453448 -1.6682957 ]], shape=(2, 2), dtype=float32) tf.Tensor(\n",
      "[[1.6619174e-01 2.6224132e-04]\n",
      " [1.4786677e-01 2.7832108e+00]], shape=(2, 2), dtype=float32) tf.Tensor(\n",
      "[[ 1.1807995  1.0002623]\n",
      " [ 1.1593584 16.170858 ]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.random.normal(shape=(2, 2))\n",
    "b = tf.random.normal(shape=(2, 2))\n",
    "\n",
    "c = a + b\n",
    "d = tf.square(c)\n",
    "e = tf.exp(d)\n",
    "print(c, d, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5q1xG24emmdu"
   },
   "source": [
    "# Exercise 1 (CORE)\n",
    "\n",
    "You can convert a the dataframe column to a tensor object like so: `tf.constant((df['column_name']))`\n",
    "\n",
    "So, consider your hotel data set as data frame;\n",
    "\n",
    "- Convert the numerical variables `lead_time` and `adr` into tensor object\n",
    "\n",
    "- Print the shape and type of created tensor objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCExrjAlmQOm"
   },
   "source": [
    "# Keras layers\n",
    "\n",
    "You already experienced by directly applying some layers in Week 9-10 but let us recall once again some properties\n",
    "\n",
    "- While TensorFlow is an infrastructure layer for differentiable programming, dealing with tensors, variables, and gradients, Keras is a user interface for deep learning, dealing with layers, models, optimizers, loss functions, metrics, and more.\n",
    "\n",
    "- Keras serves as the high-level API for TensorFlow: Keras is what makes TensorFlow simple and productive.\n",
    "\n",
    "- The Layer class is the fundamental abstraction in Keras. A Layer encapsulates a state (weights) and some computation (defined in the call method).\n",
    "\n",
    "A simple layer looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super().__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Linear at 0x7f4cd2faec70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You would use a Layer instance much like a Python function:\n",
    "linear_layer = Linear(units=4, input_dim=2)\n",
    "linear_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       "array([[-0.00755919, -0.07351511,  0.0961083 , -0.05432985],\n",
       "       [-0.00755919, -0.07351511,  0.0961083 , -0.05432985]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The layer can be treated as a function.\n",
    "# Here we call it on some data.\n",
    "t = linear_layer(tf.ones((2, 2)))\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9R_1TGs8rInz"
   },
   "source": [
    "# Exercise 2 (CORE)\n",
    "\n",
    "Consider the class definition given above for the `Linear` one \n",
    "\n",
    "- Explain the each line of code to demistfy the meaning of this class, `Linear`\n",
    "\n",
    "- Discuss the meaning of `Linear(units=4, input_dim=2)` usage above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhzuQdIuptAh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAtBj2cRs6RZ"
   },
   "source": [
    "# Exercise 3 (CORE)\n",
    "\n",
    "For your created tensors above, \n",
    "\n",
    "- Build a linear regression model in keras. The model should consist of an input layer and a fully-connected output layer. See lecture notes for details of how to create these objects, previous WS materials or ask your tutors.\n",
    "\n",
    "- Compile the model. At this stage you need to select a loss function (specified via the \"loss\" keyword) and an optimizer. \n",
    "\n",
    "- Train the model with model.fit. Pass the keyword argument (similar to previous labs). Consider small number of `epochs` like 50 for the computational time reasons \n",
    "\n",
    "```\n",
    "# callbacks=[tensorboard_callback]\n",
    "```\n",
    "\n",
    "- You might also want to split the dataset into a training and validation component via  \n",
    "\n",
    "\n",
    "```\n",
    "# validation_split=0.3\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "NQdCePvVZqkE"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgyMFi6K1gFE"
   },
   "source": [
    "# Exercise 4 (CORE)\n",
    "\n",
    "- Now create a new model with single feature by adding a fully-connected hidden layer with 2 neurons between your input and output above. Using the `linear` type of layers again.\n",
    "\n",
    "- Train the new model and comment on your fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFj2q0lj1quC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qcl7L-v812b"
   },
   "source": [
    "# Exercise 5 (CORE)\n",
    "\n",
    "To run the single-variable linear regression using keras, we can benefit from `Sequential` model idea as well\n",
    "\n",
    "- See the details of added normalization layer below\n",
    "\n",
    "- Compile the created model below and try to produce the model's training progress using the stats stored in the history object:\n",
    "\n",
    "For more details, see example given here : https://www.tensorflow.org/tutorials/keras/regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Variable selection\n",
    "lead_time = np.array(df_hotel['lead_time'])\n",
    "\n",
    "# About normalization by keras\n",
    "lead_time_normalizer = layers.Normalization(input_shape=[1,], axis=None)\n",
    "lead_time_normalizer.adapt(lead_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prELosLWIwHt"
   },
   "outputs": [],
   "source": [
    "# Created model including normalization\n",
    "linear_model = tf.keras.Sequential([\n",
    "    lead_time_normalizer,\n",
    "    layers.Dense(units=1, activation='linear')\n",
    "])\n",
    "\n",
    "linear_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Keras Model.fit to execute the training for 50 epochs:\n",
    "history = linear_model.fit(\n",
    "    df_hotel['lead_time'], df_hotel['adr'],\n",
    "    epochs = 50,\n",
    "    # Calculate validation results on 30% of the training data.\n",
    "    validation_split = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model's training progress using the stats stored in the history object:\n",
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the behaviour of training and validation loss functions\n",
    "# Consider the below function simply \n",
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error')\n",
    "  plt.legend()\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 (CORE)\n",
    "\n",
    "Consider your hotel data once again. Officially, our response is binary and supposed to be the `is_canceled` in the data set. So, we are interested in predicting a binary response here so the problem is classification. \n",
    "\n",
    "- Consider fitting a logistic regression model in keras similar to Exercise 3. The model should consist of an input layer and a fully-connected output layer. Choose one single numerical predictor, such as `lead_time` to do this experiment\n",
    "\n",
    "- Which activation function you used and how is the progress of the model fit with `epochs = 50` and `validation_split=0.3` (You can benefit from the plotting introduced before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the response with a single predictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7 (CORE)\n",
    "\n",
    "- Calculate your model predictions on the original data set (using the 0.5 threshold for the decision boundary)\n",
    "\n",
    "- Compare your findings with your ground truth (true observations for `is_canceled` variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8 (EXTRA)\n",
    "\n",
    "For the hotel data set, as an extension of above mentioned method;\n",
    "\n",
    "1. Lets create our feature matrix (including some variables) and response varible (`is_canceled`). You can think of set of predictors (suitable and ready to use predictors)\n",
    "\n",
    "2.  Split the data into training and test sets as usual. Use the test size as $30\\%$ of the whole sample herein.\n",
    "\n",
    "3. Re-create the above logistic model (the model should consist of an input layer and a fully-connected output layer) and train the model using only training data now (Remember that validation over the training set still makes sense!)\n",
    "\n",
    "4. Derive the predictions of the fitted model over the test data (unseen data set by the NN model)\n",
    "\n",
    "\n",
    "**WARNING :** Note that, these steps are followed under the assumption that all the necessary data cleaning was completed generally.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CvMFdEvvXtt"
   },
   "source": [
    "# Exercise 9 (EXTRA)\n",
    "\n",
    "By changing the NN model a bit further;\n",
    "\n",
    "1. Create a new model by adding a fully-connected hidden layer with 2 neurons between your input and output above.\n",
    "\n",
    "2. Play around the considered activation functions (adding `relu` etc. instead of logistic)\n",
    "\n",
    "3. Train the new model over the training data and consider your predictions over the test data again. Compare your predictions with the true test values\n",
    "\n",
    "4. Visualize the model's training progress similar to the previous graphical outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOM5A9n7QAc5S8QW7qOOowE",
   "collapsed_sections": [],
   "name": "week09.ipynb",
   "provenance": [
    {
     "file_id": "1rbsH-qOENezJeR5XJkpU7uhUaBuwbDM_",
     "timestamp": 1647616242630
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
