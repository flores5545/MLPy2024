{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eldA-sl8YsLp"
   },
   "source": [
    "# Week 9: Neural networks I \n",
    "[Jacob Page](jacob-page.com)\n",
    "\n",
    "In this workshop we will work through a series of model problems using (fairly shallow) fully connected neural networks. You will see how appropriately designed networks can construct complicated functions by visualising the internal (learnt) representations in the hidden layer(s). \n",
    "\n",
    "The tasks we are doing today are based on the excellent blog post by Chris Olah (https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/). There are additional links on that page to other excellent resources on network visualisation. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "As you work through the problems it will help to refer to your lecture notes. The exercises here are designed to reinforce the topics covered this week. The lecture notes include a small amount of documentation on the keras library, but please ask/discuss with the tutors if you get stuck, even early on! This may the first time many of you have seen keras, and things may be a little counter intuitive initially. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm6XqPScK-FV"
   },
   "source": [
    "# Imports\n",
    "\n",
    "We're only going to need a couple of standard libraries this week, as well as keras. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usEZAe6SYpV8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPReDTfaEg9X"
   },
   "source": [
    "The following code boxes will allow you to visualise your model training. Scroll back up to take a look once you get to a \"model.fit\" statement! (You'll need to refresh the dashboard with the refresh button on the top right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrvawtIhEnyj"
   },
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uenq9mV1ErLf"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --port=5036 --logdir $logdir\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(logdir, histogram_freq=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qeHcm_OBGhtx"
   },
   "source": [
    "# Exercise 0\n",
    "In the first part of this workshop we will work with a \"clean\" dataset, generating data from purely deterministic functions.\n",
    "\n",
    "First, generate data according to \n",
    "\\begin{equation}\n",
    "  x_2 = \\cos(x_1),\n",
    "\\end{equation}\n",
    "with $x_1 \\in [-\\pi, \\pi]$. This data will be labelled as belonging to class $y=0$.\n",
    "\n",
    "For data in class $y=1$, generate \n",
    "\\begin{equation}\n",
    "  x_2 = a + \\cos(x_1),\n",
    "\\end{equation}\n",
    "where $a=1$ for now.\n",
    "\n",
    "You should generate ~2000 samples for $x_1$ (uniform distribution over $x_1\\sim U[-\\pi,\\pi]$).\n",
    "\n",
    "For use in keras, it helps to build numpy arrays of following shape \n",
    "$$X.\\text{shape}=(N, D)$$\n",
    "with a corresponding set of labels\n",
    "$$y.\\text{shape}=(N,)$$\n",
    "where $N$ is the number of samples and $D$ the number of features ($D=2$ here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZw0Lb1wR395"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5q1xG24emmdu"
   },
   "source": [
    "# Exercise 1\n",
    "Build a logistic regression model in keras. \n",
    "The model should consist of an input layer and a fully-connected output layer. \n",
    "See lecture notes for details of how to create these objects, or ask your tutors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCExrjAlmQOm"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9R_1TGs8rInz"
   },
   "source": [
    "# Exercise 2 \n",
    "Compile the model. At this stage you need to select a loss function (specified via the \"loss\" keyword) and an optimizer. Any optimizer will do -- you could use one of the \"exciting\" ones, e.g. Adam.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhzuQdIuptAh"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAtBj2cRs6RZ"
   },
   "source": [
    "# Exercise 3 \n",
    "Train the model with model.fit. Pass the keyword argument \n",
    "\n",
    "```\n",
    "# callbacks=[tensorboard_callback]\n",
    "```\n",
    "\n",
    "to visualise above. \n",
    "\n",
    "You might also want to split the dataset into a training and validation component via \n",
    "\n",
    "\n",
    "```\n",
    "# validation_split=0.X\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQdCePvVZqkE"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csTjsKWkzj3I"
   },
   "source": [
    "# Exercise 4\n",
    "Generate \"test\" data uniformly over $x_1\\in[-\\pi, \\pi]$ and $x_2\\in[-1, 1+a]$. Use your trained model to predict the $y$ labels for this data and visualise the results. Overlay the original curves on the output -- is the result what you expect, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAUbX_p8dW_S"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgyMFi6K1gFE"
   },
   "source": [
    "# Exercise 5\n",
    "Now create a new model by adding a fully-connected hidden layer with 2 neurons between your input and output above. \n",
    "\n",
    "Train the new model and visualise the same test data from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFj2q0lj1quC"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qcl7L-v812b"
   },
   "source": [
    "# Exercise 6 \n",
    "Create a new model featuring only your input and hidden layers that were trained above. Visualise the output of the two hidden neurons for the training and test datasets. What does the decision boundary (that separates y=0 from y=1) look like in this representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "prELosLWIwHt"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dg6Br2cT9YhE"
   },
   "source": [
    "# Exercise 7\n",
    "Now we're going to try and break the 2-hidden-neuron classifier above. We'll do this by generating a dataset for which there is no way for the two neurons in the hidden layer to create a linear decision boundary. \n",
    "\n",
    "Generate two-dimensional, normally distributed data (class $y=0$), according to $\\mathbf X \\sim N(\\mathbf 0, \\mathbf I_{2\\times 2})$.\n",
    "\n",
    "For the second class, $y=1$, generate one dimensional data $r \\sim N(a,\\sigma^2)$, with $a=4$ and $\\sigma=0.2$, before generating $x_1 = r\\cos \\theta$, $x_2=r\\sin \\theta$, with $\\theta \\sim U[0, 2\\pi)$ (this is actually not going to give us a normal distribution through the annulus, but it's fine for our purposes). \n",
    "\n",
    "Visualise the results to verify things are behaving as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gm9jUj5MWNFL"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaJcxqrd_yw2"
   },
   "source": [
    "# Exercise 8 \n",
    "Train the simple neural network you defined about on this new dataset, and visualise the results on a new test dataset of uniformly distributed points. Where is the decision boundary? (This will likely differ each time you train).\n",
    "\n",
    "Of course, we should really generate test data using the same distributions as the training set, but for visualisation it's helpful to have lots of data all over the $x_1, x_2$ plane. \n",
    "\n",
    "Visualise the output of the hidden layer.\n",
    "\n",
    "(Aside -- explore how many epochs you need to train the model; 1 epoch=1 pass through the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUFOKGGSQ_Hx"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNk7rkT2soXF"
   },
   "source": [
    "# Exercise 9 \n",
    "What is the simplest modification to your architecture that would be able to classify this dataset? \n",
    "\n",
    "Attempt to verify your predictions following a similar workflow to the above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4jKNnI3ecn9"
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CvMFdEvvXtt"
   },
   "source": [
    "# Exercise 10 (optional)\n",
    "Now redefine your data to overlap by setting $a=2$.\n",
    "\n",
    "Complicate your network a little -- add extra layers, more neurons etc. Train and watch validation loss. Can you make your model overfit? How might you counteract this? You may want to read the keras documentation a little to decide exactly how to implement your ideas (e.g. custom losses, dropout layers...)\n",
    "\n",
    "You may want to refer to the keras documentation, e.g. https://keras.io/api/layers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-xxX5PkS7uE"
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week09.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
